hello everyone thank you for joining us today and welcome to this joint Microsoft AI for Earth and WI webinar on using artificial intelligence AI to tackle global environmental challenges and I think Lauren wave told you that we are recording this today we'll make that available on WIC event page later on so I'd like to just thank Microsoft and double your eyes institutional donors the lemons mystery of Foreign Affairs Royal Danish Ministry of Foreign Affairs and the Swedish International Development Corporation for their generosity and making all this work possible so I'm Janet ranganatha I'm the vice president for research data and innovation at wri and we're a mission-driven organization not-for-profit and we work on these seven virgin little challenges so food orest water climate energy cities in the ocean and put it on my brain and we made harnessing the data revolution a ore strategic priority and actually this year we launched our data lab thanks in part to a generous support at the Patrick Jane govern Foundation and you're gonna hear from Allen the new global director of our data lab he's going to be joining one of the parent panels later on so in today's fast-paced world of the data Revolution we kind o become accustomed to having the latest acts and figures at our fingerprints perhaps to track and monitor our own health the health of our investments and I'm actually even being someone pointed out to me an app called run pee yes run pee that actually gives you information of a good time to run to the bathroom during a movie so that you won't miss the biggest part so I can tell you I haven't tried it yet but maybe when I'm back in the movies I might but when it omes to the health of our planet we mostly don't have the information we need and it's not because it doesn't exist in fact that was the problem when I started at double your ID from 2 to 2 decades ago but today I'd say the world drowning in data yet starving for actual data and insights and too often Data initiatives are supply-driven edges of data looking for users now W right we start with the problems whether it's something like how do we prevent illegal deforestation in ountries with limited capacity and boots on the ground or you know how do we reduce urban heat he affects given rise in temperature or where do we actually best invest our money in reforestation to maximize our climate mitigation benefits and you're going to hear about some of these later on and then we figure out well how do we actually bring things like AI and data and other models to solve those problems so if we could just move to the agenda slide to give people a preview of what's going to happen today that would be good and just move to that slide you're gonna first up here some lessons you know these five lightning talks o projects that we actually support it with this AI for earth and WRI Innovation Park and they're going to talk a little bit about what worked you know what were some of the struggles they ran into and what are their hopes around how this data is going to be used and the results and there can be ollowed by very engaging on this very promising expert panel and then we're kind of conclude with an open Q&A session so let me just sort of anybody saying it's been an absolute pleasure working with the Microsoft a i4f team they came armed with the technical resources cloud and open source tools that we could only imagine WI and double I had the noise you know the deep knowledge of the world sustainability problems so we we jointly supported these five teams that you can hear from we met with them regularly we helped them truck troubleshoot the roadblocks along the way and it would be remiss o me not to give a big shout out here to Laura malakooti Valerie and Gregory Taft and the DA variety and Bonnie Lee from the Microsoft team because they truly did do all they have to heavy lifting or beginning to end so enough for me I'm going to now hand over to Dan Morse Microsoft's AI for program so Dan take it away thank you very much hey everyone I'm showing my screen I assume something to tell me if you can defy me there mumbling and then we can hear me or i you can't see me so I'm Dan many of you I run the effort program Microsoft's and I'm gonna give you a little tour through 3f earth and and where where we've been where we're headed and most importantly what the help we need from Wi and from the audience we're talking to today and also pro tip anytime you ever give a presentation start with pretty pictures of animals it puts everybody in a good mood you can go on and give a terrible talk and as long as you have pictures o pretty pictures of animals at the beginning and end of your talk everybody Allah come away happy so I ever earth started that a couple years ago primarily as a grants program we are still very much a grants program we give away both esra resources and inancial support to projects at the intersection of machine learning and environmental sustainability we have I an this slide is by definition it's out of date the instant we make it every time if we try to give keep track of the number of grants we have but let's go with over 600 grants in over 8 ountries and you can learn more about our grants program at the URL here so this is still very much the important part of our program in this five I wish I could tell you 615 great stories about all the great work that our grantees are doing I don't have time for that in this ive minute talk but if you want to hear some stories of our grantees including some of the WRI folks who will be speaking today go check out this URL which is our relatively new grantee gallery at a kms AF for grantees and I take some time to read through all the stories this really the heart has been the heart of our program is the great work that our grantees are doing I'm still go check this out and learn the kind of work that we support anyway so we kind of grew up as I grants program as we worked with our grantees we saw a ertain number of very common problems emerged on the machine learning side and really started to not only be a grants program but also a technology development program specifically around accelerating conservation workflows with machine learning as we saw common problems emerge amongst our grantees particularly problems where environmental scientists especially ecologists even more especially spatial ecologist and wildlife ecologist spending lots of time doing things that are time-consuming and often tedious and we saw huge opportunities as our grantees did to use AI emission learning to help make more efficient use of their time and help answer conservation questions more quickly so we added to our Grants Program a emerging machine learning technology development program ocused largely on a land use assessment and wildlife population assessment since these were both really consistent themes among the projects our grantees are working on and it's good this talk is being recorded because I'm flooding looding your eyes with URLs to go click on to learn more about our work in this space right now because I'm not going to go into these in detail but all other projects we worked on as we started to add a technology development component to our program we're really focused on using machine learning to help onservation scientists spend less time licking stuff and doing tedious data annotation tasks and more time focusing on conservation and then again as our grants program and now our budding technology program continued to grow we saw another key gap in our program specifically around large geospatial datasets I'm so particularly in environmental science many of the work much of the work that many of you do as you know depends on very large geospatial datasets so in certain spaces we can say here's the cloud here's cloud resources here's tools that we've built to make your life easier bring your data to the cloud and we will help you do machine learning tests in that data that's not true in the space we work and we depend in particularly in particular on remote sensing data that can scale into hundreds of terabytes and even petabytes and it became increasingly important to support our grantees to add a data pillar to our program so we ontinue to be a grants program we ontinue to develop technology for accelerating conservation workflows but hosting large geospatial datasets for this community has become an increasingly important part of what we do so you can see a bunch of the datasets we're hosting they don't all it on one slide but if you go check out our data page you'll see a number of the data sets that we're already hosting and a number of important data sets are in progress behind them and we really felt this was the most important next stage of our program to fill the needs that we weren't at the time meeting for many o our grantees who depended on these data sets and that sort of gets us close to where we are today so our data program has been a major focus of AF Earth for the last year or so we are hosting several petabytes of data in the cloud but it turns out that unless you have a PhD in distributed computing doing stu in a practical amount of time with petabytes a global scale data isn't is still not easy we're working hard to make it as easy as possible but it's still not easy and that really brings us to where we're headed now which is in addition to all the pillars of our program that we've established over the last couple years grants technology development and data we've announced our intention to build a planetary computer over the next year or so meaning a set of expanding our data program that I talked about in the previous slide but also really putting into place some distributed computing tools and spatial analysis tools that make it much easier or the environmental science and ecology communities to work with data at planetary scale that requires massive amounts of computing that isn't easy to manage even if you do have a PhD in omputer science and so really were we're focused now is adding an important infrastructure pillar to the grants data and Technology Development pillars that have been the mainstays of our program or the last year or so and then lastly that brings me to what we need from all of you so we're at the early and we're even still at the early end of our data journey and certainly at the early end of our infrastructure journey and part of the reason we're excited to engage today engage with this community in general is that we need that we are a great time to take feedback from this ommunity about the right ways that we kind of make your lives easier by making it easier to work with large geospatial datasets so no detail is too small we want to hear from you about the geospatial datasets that you use the we know that geospatial data folks have a lot to say about data formats and data processing we want to hear about that we want to hear about the tools you use the programming language you use and the experiences you may have had with the data we're already hosting or the data that other clouds are hosting because really our major adventure over the next year or so is to take all the data we're hosting and make it more accessible to the community we're talking to you today to the environmental science and sustainability ommunities more broadly of whom are AF earth grantees continues to be our most important sample and representatives o that community that's everything I really want to say about the AF earth program today this is how to find me i you have any more questions if you want to send me email about all the questions asked on the previous slide fire away no detail too small and now I will turn things over to the grantee lightning talks thanks everyone you already hi everyone well thanks Dan and Janet for a great introduction so this is Christine Lister and I'll be covering the first of the five AI Birth projects predicting future deforestation and the Democratic Republic of the Congo next slide our project specifically focused on primary force primary forests also known as old-growth forests are natural orests that have been unaffected by humans the Congo Basin holds the second largest primary tropical rainforest in pard\pardeftab560\sa40\partightenfactor  1\b\fs28 \cf0 the world and these primary tropical pard\pardeftab560\slleading20\partightenfactor  \b0\fs24 \cf0 rainforests are crucially important for arbon storage and biodiversity next slow about 60% of the primary tropical rainforest is held in the Democrati Republic of Congo or DRC however the primary forest loss rates in the DRC have been an all-time high in recent years about 84 million people live in the DRC and people clear the primary orests for shifting cultivation also known as fashion burn agriculture mining and hunting it's crucially important to understand how the patterns of primary orest loss can be can be used to prioritize conservation efforts and our idea was to use machine learning to predict the future patterns primary orests locks to prioritize areas for onservation and understand the drivers of deforestation next slide so to do this we use data from the hansen annual tree cover loss which is available at 30 meter resolution and shows the year that pixel experienced recover loss from two thousand to two thousand eighteen we focus our study period on 2012 to 2017 and collected 18 predictor variables that cover think drivers of deforestation including accessibility land abuse management and terrain the map on the right shows a subset of our predictor variables which shows how complicated the predictor variables can get in the DRC and and highlights the importance of using the AI model which can understand the omplex relationships between our predictor variables so we gathered our predictor variables for around the 2012 period to train our model to use the predictor variables to learn the patterns that influence the risk of tree over loss so our model the goal of our model is to be able to distinguish between a pixel that did have tree cover loss and one that did not so once we're happy with the performance of our trained model we then update our predictor variables to a more recent year around 2017 to feed into our trained model to then predict the future risk of primary forest loss for the next ive years or 2018 to 2023 next time so rom our model we have a number o different results that can be used to onvey to stakeholders the patterns o primary forest loss and the expected hanges we see we expect to see in the next five years so the biggest one is the one on the Left which is a risk map of primary forests lots that we onverted to an ordinal map - that is easy to convey to stakeholders such as government officials and land use planners the areas in yellow are low risk areas of primary forest loss and the areas in red are high risk areas o primary forest loss these high risk areas can be used to demonstrate which areas are at most risk for primary orest loss and can point government officials where to target conservation efforts from our map we affirm our model we can also predict the expected area o primary forest loss in the next five years and we see similar high rates o primary forest loss that we observed in the past couple years as well next slide so the takeaways from our project is that machine learning can be used to predict uture patterns primary forests loss it an also be used to understand how the predictor variables that we included can affect the patterns of primary forest loss and can be used for land-use planning scenarios to see how a new protected area or a new road will change the risk of primary forest loss from our model it's important to acknowledge that this analysis would not be possible without data for our predictor variables rom which we took mainly from the DRC orce atlases and the training data for the Hanson annual tree cover loss next slide so the next steps for our project is that we are working with the double your eyes central Africa offices to ommunicate our model to the DRC government officials to encourage its use in land use planning efforts and we're also exploring potentially expanding this analysis to other ountries in the Kanto Basin we'll also be uploading all of our code along with tutorials to the WRI github so that this type of analysis can be done anywhere else in the world and maybe you can get an AI for Earth grant to do this type o analysis in your country so to learn more about our model you can reach out to me christine lister at WRI dot org and thank you for listening hi everyone I'm Logan Byers a research analyst and of your eyes climate program I'm very happy today to have the opportunity to be sharing with all o you online so today I'll be quickly talking about our work on estimating power plant electricity generation this is work performed by a fairly small team here at WI that's the next slide please right so from environmental perspective power plants are really important to haracterized by burning fuel electri electricity production accounts for 35 to 45 percent of global co2 emissions they also compete against other needs or freshwater including public drinking sources and irrigation supplies and on the opposite end not really shown here are renewable wind solar facilities that are experiencing really rapid growth as they come online and are in many cases offsetting older dirtier plants and lessening these environmental impacts so satellite aerial images like that shown on the right side are really the new bread-and-butter of financial and commodity trading ompanies there's entire analytics industries being built around high resolution monitoring of the energy sector and they have daily or hourly views of what's going on worldwide unfortunately that information doesn't really trickle in to the public sector and publicly available information very much even though even through you know aggregated or reduced resolution views and so what this really means for the energy sector as cord as it is to the economy in our society is that there's actually a really great need for open information we're actually in a kind o a data poor situation next slide please so in 2018 WI published an Open Access database of the world's power plants we pulled information from hundreds o sources covering 30,000 power plants we over lots of fuel types and we really tried to be very broad we have fossil uel and renewables nuclear biomass et the goal here was really to centralize hundreds of sources into an easy-to-use spreadsheet have better interoperability across countries and fuel types we added a really key value which is this Geographic mapping and we also have a lot of openness in our sources and information so all this will come back and very quickly so we did have a hurdle though which was great differences in data and it's really driven by geography so as I said earlier our goal was to have really broad coverage we have something like 85% of global capacity to generate but we're really deficient in something which is the actual electricity generation so to make an analogy we have apacity which is like an instantaneous measurement like speed of a maximum speed of a car it's something that is a potential what we're missing those really generation which is like the total miles traveled by a car it's the real log and history of what happened and so something like two thirds of the power plants in this database don't have reported generation and information and not having generation information really limits our ability to understand and describe these environmental impacts that I mentioned earlier so emissions water usage are both orrectly controlled by the amount o generation in a power plant next slide please so for our a our first project we sought to develop four models or estimating power plant generation at the annual level plants of each of these different fuel types generally behave differently which is why we said set up our models like this and they have different controls on their behavior so there are four plots here really quickly there's a horizontal axis representing the true reported generation there's a vertical axis representing our estimates of generation there's the one-to-one 45-degree line which ideally all of our dots or all of our training samples would fall on that line and we are doing really good estimated really good estimations and matching reality and zero is in the bottom left corner that don't matter so much below each fuel label there's also a mean absolute percentage error which is a measurement of our expected error for the whole set of plants so some top level messages are that our solar and wind models which were based on near surface atmospheri data performed very well wind and solar generation intensity of wind and solar generation are really controlled by local environmental conditions we had worst performance on hydro which used watershed accumulated runoff or precipitation runoff as the main model input the input itself has some uncertainty and there's also many places where hydro generation is very strongly ontrolled by regulations and then we generally had fairly poor performance on our natural gas work we interpret this to be associated with tight integration gasps plants with the system level dynamics and those really dictate how natural class natural gas plants can run and there's also market and fuel price onsiderations that we probably didn't apture very well so onto our last slide here going forward with this work our detailed descriptions of our methods and data are available in the tech note on screen this is currently published it's available on double your eyes website and available to download we are presently incorporating this estimated generation information into our database it will be published and I should also say this database is open access and make sure that was clear open access public database of power plants and including generation in there we're also in the very early stages of collecting information that will make it possible to estimate water usage across these many geographies so with my time closing really quickly I will thank you for your attention hello good morning my name is Beatriz Cardenas and I'm I'll be just introducing this project which is a very good example of data experiment of the data experimentation program which is illing the gap for air pollution and here we will show you how using artificial intelligence can help us to improve the air quality data access and particularly talking in Sao Paulo we will be talking about PM 2.5 or one o the pollutants that we all know that aused a lot of impacts of on health and environment but just an example because that could be done for other pollutants and I will pass to my colleague Walter and the Simoni who will continue with the presentation good afternoon everyone I'm all from the W ride Brazil team thank you everyone think of their thirties we're trying to ill in our important gap here in Brazil we are a country with 211 million people with very limited access to data and knowledge on air quality on the left you an see a map on population distribution in Brazil and the red dots that red ircles represent how many stations we have on the right you can see the metropolitan region of San Paulo which holds 10% of the Brazilian population so we're talking about 21 million people with only 28 air monitoring stations which definitely do not give us enough information on what is happening on the ground they definitely did not we definitely do not have stations around the periphery so in local income regions so even though this is the best covered metropolitan region in Brazil when it comes to air quality monitoring there it's still a lot to be done and we need better data and we need more data so we try to see whether a I ould be used to fill in this data gap starting with simple we have some we're not on data and moving on to other places hopefully to see if the I can help us next why please well here what we do what do we have we all know that iris hold optical depth it's measured from satellite which can give us actually the information of some pollutants in this ase PM 2.5 so usually what you do is you have the coverage and you can use data from the ground monitoring station as Walter mentioned to make a linear relationship actually that's that's been done but there is a lot of potential to improve the data so what do we do well we consider all other information such as land use data but that could tell you so much about the land use change we we use the satellite data that is available then has a great coverage we use the surface data from PM 2.5 minutes in this ase and we also include that meteorological variables so all of that make a model which is nonlinear but it gives you a better better integration and using artificial intelligence you actually have the outputs of the comfirm of the information and they correlate the relationship between this satellite data and the ground monitoring station and in this case you are getting better information for a more a bigger extension coverage so that's that's what how we use the artificial intelligence but they're up to you [Music] so to make a very long story short we were able to get very good estimates rom satellite data on the left you can see the training sets and the validation sets we were able to get very good estimates for the metropolitan region o San Paulo there is a there is a hallenge which is we need graph mandate in order to build this model and as I mentioned our a lot of places throughout Brazil have the ground data to offer us so we are looking to how to overcome this issue whether through the use o mobile sensors whether through the use of other sorts of mobile stations but how can we start building upon this project to have better data so we can move into enacting policies next slide please the state government if it embraces the solution state governments throughout Brazil are responsible for monitoring you can have a lower cost solution rather than implementing stations throughout the country a national aq platform can build and bring this data giving you transparency so local groups researchers in academia can start building on this data and start looking into correlations and studies and whether to see policies being implemented throughout Brazil or not we an improve estimates and forecast orecast is something that is not often done in Brazil and regarding air quality and is probably one of the next steps that we're going to look how can and we make sure that cities can forecast and better help the population adapt to periods of worst air quality whether it is to behavioural changes or otherwise and finally how can we structurally use low-cost sensors to help fill in this gap in to what extent the low-cost sensors can feed into the AI model and an help us get better estimates and even better predictions for our quality without 24 or a 48-hour window there's a lot to be done and we're very excited to explore many many possibilities and how these new technologies can help us work in a very data poor environment here in Brazil in a very needed area which is air quality next slide if you guys have any questions you can get in touch with Rhys with me and with the next general amps who are responsible for doing a lot o the work on the satellite images for us modeling thank you alright kind of rihwan I'm back again this is Christine Lister talking about the second horse-related AI for Earth project map and global climate mitigation potential from natural forest regeneration this was a collaborative effort across the World Resources Institute the Nature Conservancy the NASA Jet Propulsion lab and other organizations nextslide restoration of course has been identified as a necessary and feasible way to remove carbon dioxide from the atmosphere in order to combat climate hange as trees grow they sequester or remove carbon from the atmosphere and store it as biomass this rate of carbon sequestration can be used to estimate the potential of carbon mitigation potential of naturally regenerating orests to in order to combat climate hange so right now the best estimates of the carbon sequestration rate and young naturally regenerating forest is rom the Intergovernmental Panel on Climate Change or IPCC and it's only available at the biome level this is what countries used to estimate the mitigation potential from naturally regenerate forests to use in their national climate mitigation and plans so this means that one number is applied across huge regions such as one number or all of the Amazon or one number for all of the Northeast United States to estimate the rate of carbon sequestration from naturally recurring orests so our hypothesis was that using machine learning and a series of four of ground truth measurements a forest regrowth we an refine the spatial resolution and improve estimates of the climate change mitigation potential from natural forest regeneration next slide so in order to do that we took a series of like I said ground truth measurements of natural orest regrowth rates from a study and national inventories along with 66 environmental covariant layers covering things that impact tree growth like limate soil and topography to feed into a machine learning model to train on our ground truth data in order to predict over areas of begun five days on which results in this map at one kilometer resolution where the green areas represent higher rates of carbon sequestration and the kind of yellowy areas are lower rates so we applied our model over all forest areas and savanna biomes but the purpose of our map is to say if an area was cleared and a forest would naturally regrow if you allow that orce to regrow what is the average rate of carbon sequestration in that area per year so next slide the our map is designed to be used in combination with restoration potential Maps this one is rom Griscom at all so we filtered our rate map to the Griscom at all restoration areas to show what how actually do we expect for us to grow in restoration areas and this is what can be used by governments and land use planners estimate how much natural orest regrowth can contribute to their limate mitigation plans next slide so we compare our rates to the IPCC estimates that I showed in the first slide and so the open circles are the average rates that we predicted over the biomes and the closed circles are the IPCC estimates where the x-axis is each biome and continent pear and molasses is deep of ground rate then the bars indicate the range that we predicted the rates over that biome so the minimum and maximum that our model showed over that time so we found that the IPCC actually underestimated the average sequestration rate across the biome slide 32% and even more importantly it underestimates the variation across the biome so the fact that we have these large bars shows that there's a huge variation and the rates across the biome and that means that one number cannot completely capture the variation in the biomes next slide so the next steps our research group submitted paper which was accepted for publication by a research journal so keep your eye out or that so once our publication is or once our publications published governments can use this map to determine the ontribution and natural forest regrowth to their climate plans and Lanny's planners can use our map to target areas with high sequestration rates for restoration so our map will be available on global orest watch and we're also going to be developing an API to be shown on the AI or gallery so users can get estimates of the sequestration potential from their restoration areas and you can direct your questions to Nancy Harris at an Paris WI dot org thank you you you hello everybody this is Eric Makris I'm gonna share the project focused on mapping urban surfaces and how that relates to reducing the impacts of heat in our city's cooling our cities so this is a project with many of my several o my WI colleagues listed here as well as in partnership with the global cool ities alliance the city of Los Angeles and George Ben wise - professor at University of Southern California next slide please so he is really important for not only our cities but how humans live on the earth in general they it impacts just about everything from agriculture to air quality to mortality it's actually one of the highest highest causes o mortality related to climate change and it also has significant implications social implications and they're closely orrelated with issues of equity in ities and outside of cities if we look at the impact of heat just purely from an economic perspective with the increasing urban heat impacts associated with climate change oh there's projections that heat alone will decrease GDP by nearly 2 percent in 2050 and almost 6 percent in 2100 next please so as you can imagine many cities are increasingly looking at ways that they an not only understand the problem they have associated with heat in their ities but to identify specifically what sort of mitigation strategies how can they address heat and how can they reduce the heat the impact of heat on their citizens and on their economies so among the options are around integration strategies many of them are about the physical form of cities and how cities are designed and built I've shown here three different really key strategies associated with mitigating urban heat trees and vegetation and two different kinds of cool surfaces rooves and also streets and you can see they're kind of two different scenarios in Egypt for each of those about how a person would would experience heat on a street associated with cooler versus higher urban surfaces so what a lot o ities are looking for is that they know this is that these are options that are available to them but there aren't really good methods that are repeatable scalable and that can help cities measure change over time and target their actions in terms of both places and time kinds of interventions that they're taking so that was the challenge that we were working to address with this project next slide please so what we did is we developed the prototype for the first globally scalable AI and remote sensing based methods for quantifying these three different kinds of critical urban heat mitigation strategies so we used known values around urban surface reflectivity and also tree canopy cover to create models to be able to predict that across much greater areas using high resolution satellite imagery next please so the model that we've produced so far is ocused on the United States and we hope to be able to scale this up at the other geographies into the future this is a map there's an image a satellite image of a neighborhood in Los Angeles where you can see buildings trees a variety o different things but without being able to save Memphis it's hard to know what's happening here and what this means for urban heat so our model work to do just that next please so first you can see now the results from our surface albedo model or streets so how Vito is the unit of measurement or solar reflectivity in this map lowest albedo means that basically all heat is absorbed and highest albedo means that it's all reflected and the darker that the redder colors means that more heats being absorbed the bluer colors the more is being reflected so you can see some here the lines on the streets and the variation and reflectivity of those different streets next please similarly we produced a map o reflectivity of roofs and you can see significant variation in terms of the reflectivity of different roof types and different building types around the city next please and then a third model we developed was ocused on urban tree canopy cover and again using these three data sets in in ombination cities can get a much more nuanced understanding of a block-by-block a building by building you know a by tree understanding o their current status of mitigating urban heat and where they might prioritize additional actions next please so this is one example of how we've begun to use this in partnership with city of Los Angeles the the model we've developed we've produced results for several different benchmark years in Los Angeles and two of those years were 2016 and 2018 right before and after in a pilot that the city implemented around cool pavement in a couple of different neighborhoods so make sure you keep an eye where the these circles are and then next please so this is the results from our model or these two different street segments in 2016 we're a cool pavement coating was applied to be trying to change the albedo and to demonstrate what that ould mean for it for the neighborhood this was done in each district of the ity so this is the results before that intervention and they keep an eye on that same circled segments those two ircled segments next please and then 2018 so these are the results from our model and you can see that in both cases and pretty much all the cases we looked at there wasn't a change in albedo and that you don't like top so that means that surface is no more reflective its retaining less heat and cooling the ooling net and you know the average experience in that neighborhood around that street these are complicated issues but this is you know just one example o how a model like this can help the city to measure its actions and the impacts of those actions additionally potential applications include eating a citywide or a neighborhood scale assessment of change over time in these different critical piece contributors to your main heat mitigation and then also you know like this similarly the LA has a program where they provide incentives cool cool roof materials in place so they can similarly use that method like this to track the implementation of that program and then fully understand if the implement the true implementation and the impact of the programs like that next please so yeah thank you for your attention and this is more information about me and my my colleagues additional documentation is available on and github and we're continuing to work on these different models to make them more relevant and to make them able to be applied in additional on geographies outside of the United States improving their accuracy and then making it so that we're able to deploy them in a way that they're regularly updatable as new imagery becomes available so thanks for your attention thanks Lauren and thanks up fer to all the presenters for talking about some o the work that we're doing so I believe that I'm unmuted I am just a sudden I'm lead our global air quality work but I've also as a former game theorist had a long-standing interest in information data and how it changes the possibilities for both collaboration bargaining and social change so just really quickly we're about to switch over to the panel but I did just want to point out that for information about any of these projects well the video from this will be posted and we also if you have interest in finding out more about any in particular projects do go to this survey link and let us know and we'll onnect you directly to the teams that have presented we'll also in the end o this session have some time for Q&A with both the presenters from the different projects as well as the panelists but I don't want to take up time from the second half of the program which is a great panel that I'm really honored to be chairing so I'll introduce the panel and shut up very quickly and turn it over to them to talk about AI and some of the ways in which it helps tackle global environmental challenges so we have Claire Malamud who is the CEO o the global partnership for sustainable development data we have Steven Brumby who is the senior director for data visualization at National Geographi Society as well as a senior fellow at WRI and a co-founder of decart labs we have Bonnie lay the head of global strategi partnerships AI for Earth Microsoft and we have Amon Rama Cherokee who's the global director for the data lab WRI as well as the senior fellow at the back Center for social innovation so I don't want to take up time from the panel but I do want to exercise my prerogative as the moderator to ask a couple o questions first before turning it over to the audience and by the way the odd or the audience with the this meeting ormat if you can type your questions into the hatbox the QA Laura malaguti nee will be aggregating some of them and we'll get to as many as we can at the end o this the first question I had was really inspired by some of the presentations that we saw and the emphasis on use ases I wanted to expand that a bit to have to ask Claire to talk more about how we can make sure that the future ongoing and future investments in AI and data collection and really understanding environmental change engage with users and engage with uses how your organization is a global network working together to ensure new opportunities in the data revolution are used to achieve the sustainable development goals how do you do that how do you systematically engage with users and what are some o the lessons that all of us should should take from this so over to you Claire Thank You Jessica and thank you all for those really really interesting presentations incredibly inspiring to my mind data and information is really potential you know it's what it offers us is the potential to improve policy decisions to improve accountability to galvanize civil society efforts to make hange but the data itself is not the hange the data itself and the or the you know the the information that we can reate through manipulating data with AI is potential to change but is not in itself to change for the data to help to become a tool for that change for better policy for better outcomes you need a whole series of other steps and that's all as you said Jessica about how the data is used and I think you know often as we're sort of thinking about data clearly the kind of what is possible now to do with AI and through manipulating the kinds of technologies that we have available is sort of so exciting that we sometimes forget about the other frankly much less exciting stuff that you have to do to make it actually useful you ase demonstrates that something can be used demonstrates that it has a potential value but to actually make that useful I think in our work to the global partnership for sustainable development data is all about taking things like these amazing applications that we've seen here you know and all o this sort of many tools and platforms that exist and actually working with users with people with real problems in the real world and making those onnection between the tool and the problem and seeing how we can make those things useful and there's basically three broad sort of areas of the thing kind of things to think about in turning any of these things to useful to information which is used to change things the first is of course the human element and I know that a lot of you are doing that already talking to people understanding their problems working out who is it who might want to use this how would it need to be presented to them getting people excited talking at things like this getting people excited about what you're doing and then of course engage much more in a much more focused way with individuals who have specifi problems but even that is not really quite enough having some one person say in the government who thinks that this idea is great and is really excited about it it still doesn't actually get you to the point you want to be the second second thing to think about as institutional how can you know the person who's developed the thing and that that user that person who's excited about it work together to actually reate the kind of institutional architecture that can help to make things happen governments in particular are kind of slow cumbersome beasts that need a lot of hand-holding to to get things to happen so you need to think about the institution about the governance about the politics all the stuff that you know the wheels grind slowly and it's not as fun as developing a new map or a new app but it's incredibly important and the third thing is the economic you've got someone who wants to use it you've got an institutional you've got an institutional framework in which it can be used that doesn't mean it will necessarily be used unless you've put money in you've got people who trained you got the money infrastructure and so on so there's kind of economic element to all of this as well how much does it cost and where does that money come from thank you thanks so much I want to pick up on one point that you raised Claire and then also turn it over to you I'm into expand on that and that was the point about the the data is potential and I think one of the things that comes up when you look at who benefits from this potential and it is quite significant concerns about equity and equity of access to the power o artificial intelligence one factor is that some of the power of pattern recognition depends on having training data sets in the case of the air quality work see that the power of being able to use the goes satellite depended on having ground referencing information to be able to calibrate and to learn some of the patterns so I wonder I mean i you could both speak a bit more about W our eyes approach to making sure that we onnect investments in data to two uses and two high-impact uses but also about this point of making sure that the knowledge and the potential is actually open and accessible what are some of the the efforts investments initiatives that you see is necessary for making sure that this potential is equitably distributed as for that question that's such an important and actually a very timely question I think when you think about equity and data and equity and AI and and and how we drive impact you have to think about a couple o things one first you think about sort o who and how data has been collected in the first place right and so there's a term of art for this that's dating data provenance so whether this data come rom how did we get it why didn't we decide to collect this data in the first place and I think it points to you know some key partnerships that wri has with Microsoft specifically in their planetary computer initiative where you're talking about really important and curated data being stupid in the right way and so that's irst is you have to think about appropriately curated stewarded and authoritative data and wri has been doing this for a very long time and bring this brings this practice into the data revolution second you have to think about this onversation around who's creating these algorithms that are having this impact and so when you talk about who's reating is these algorithms you have to be cognizant of the partnerships wri does it has to be cognizant of the partnerships we have with the Microsoft we have partnerships with academi experts in AI so we're very deliberate and who we partner with and how we go about identifying algorithms that should be applied in the spaces that we work in these global challenges for sustainable development and then thirdly and this is key you saw in all those amazing presentations around AI for Earth that they all had a focus on a particular region in a particular space with a particular problem in mind now what's key is once we get these analyses these really powerful analyses we have to go into those communities we have to partner with the local organizations on the ground and WRI has been doing this part the the on-the-ground work for decades and we're bringing this into the AI space which is taking those analyses going onto the ground and getting eedback there has to be this loop where we're taking this data we're taking this analysis we're taking this work and going in and engaging people on the ground and saying here's what we're seeing here's how we're seeing it let's get feedback and making them a part o this this should not just be data and some nebulous algorithm that's brought together and then we are pushing some sort of solution right Claire talked about the potential you will never reach the potential of data in AI if you're keeping it within that bubble we have to sure in the chasm close the chasm between AI analysis and the needs and the people on the ground and Deborah is in partnership with my trophies is doing at in spades thank Simon and I think that there are a ouple of points that that Claire and Amin have both raised about the need to take the analyses to users to ensure that they're co-created with users to ensure that they're transparent and understandable Bonnie do you have I mean as the director of global strategi partnerships and AI for Earth you're obviously in the thick of making some o these ecosystem level investments in ensuring that the potential of data is used directed and accessible to more ould you say a bit not only about some of the things that you're you're doing so I think Dan gave us a good intro into that but also looking ahead at the horizon what you see is some of the most exciting technological developments from the perspective of ensuring wider heaper faster access to the data that an help motivate and enable a response to environmental change thanks so much Jessica and I think that you sort of hit on it exactly which is we're looking at incredible projects that were able to support through AI for Earth over the ourse of the past three years looking at some of this great innovation and you got sort of five tastes of this today through these WRI AI for Earth projects that we just had a chance to listen to and in our head we're really thinking through how do we make sure that these great developments that are happening in these projects in the more than 6 projects that we've supported now how an we make sure that they're scaled in a way and they're made available in a way so that they're going to actually going to be useful to a larger crowd a larger community and so I agree a ton with what Claire had outlined before thinking through how do we think about the needs of our users and also an Allman mentioned about thinking through how do we make sure that this is made available in an equitable way and going to keep you getting to the communities that typically are not necessarily the large companies or the large banks or institutions but going to be some o these researchers nonprofit organizations community organizations and so a lot of that thinking is what was in our minds as we were thinking about this next stage a little bit of what Dan man mentioned around building the planetary computer and so I'm sort of answering your question from a different format because I'm not going to be listing a big list of sexy exciting technological innovations that you know we're we're saying we want to bank on this we're actually do spending a lot of our effort now is going back and thinking through well we have some great existing organizations such as WRI that are working on and investing in some o these technologies they're going through the effort of making these projects open source and available they have they have onnections with certain communities but what are some of the technical infrastructure pieces that we could put in place to be able to be helpful to these organizations to help them scale what they have developed and help these other groups not start over from strip rom scratch not have to invest in data scientists building AI models from scratch but being able to take advantage of what has been developed and work on using their expertise to be able to localize their solutions make it specific for their use case for their ommunity cases and so that really is over the course of this next year plus where we're going where we're making a lot of our investments in building that planetary computer infrastructure in order to centralize some of these key data sets centralized and make available some of these open algorithms and invest in making a user interface that's going to be able to allow all of these different users from across the world to be able to interface in a much more intuitive way thanks so much funny I'm gonna try to go back to Steve Steve can you just let me know if the line is open with you oh yes great okay so we have Steve back so Steve you have an accumulation o questions from engaging with users to ensuring that we the access to the power of AI is more equitably distributed and open and what are some of the question that I had had or you originally was what are some o the deeper almost technology infrastructure institutional infrastructure investments that you see as necessary for ensuring that we really do get to a place where AI can be used to motivate and enable equitable and a useful response to some of the environmental change we're seeing is that there's some structural problems in the whole research community that that act perversely to limit equitable access to AI technology so explain so for example people have done a great job Microsoft has done this Google is done this to to make algorithms and software available for people to do AI experiments and more than experiments and all the major cloud platforms have made large amounts of for example earth observation data available in standard ormats open format so that people can get access to raw data the thing that's lagged especially in the Earth Observation space has been access to training data because if you don't have training data you teach the omputer to do anything and but the but where the perverse incentives comes in is that the is that a few research teams that have gotten access to high-quality training data use their training data to ensure that they they you know been successful the grant applications now there are some cases in computer vision outside of Earth Observation where people have put together large training data sets that move the whole field orward ype like the image net data set reated by a professor at Stanford and the and the space net data set for just the US government and there's a real need for those types of training data sets a human markup of large amounts o data which is to make somebody has to stand up make those types of datasets um that are reely available altruistically so that lots of research teams and even individuals contracts balance and so that's one of the things that we've been ocusing a lot of National Geographi oming up with training data sets that an be used to help people create an observation based machine learning products and then and then focusing on making sure that we collect the training data and providing that training data rom all over the world so that again you don't you don't get a situation that urrently exists with this quite frankly a large rating data yes reasonably good amounts of training data or a Qi priority regions outside the US like selected places in Africa and a few places in South America Australia but typically there's just no data once you're outside the US and Europe so there's a that type of investment in reating training data sets that are open as well as the raw data in the AI is something that I would love to see a lot more of in the community thanks Steve hey that actually leads very very nicely to some of the questions that are coming up in the Q&A chat which were actually there was a question for the panel but I want to come back to you Claire and start with you which was really about what will it take to create greater ollaboration across organizations both in potentially joint investments in the kind of open training data sets that Steve mentioned joint investments in infrastructure and standards and ensuring interoperability what do you see as ways to encourage and continue and deepen collaboration that is starting and what do you see is some o the obstacles to that and we'll follow up that was a sort of a joint question we'll follow up also with a specifi request for for Bonnie to speak to views on Microsoft working with Google AWS and some of the other big providers out there but first Claire what would it take to make this happen more often and the frictions thank you I mean I tend to think about questions like this in terms of incentives you know what are the incentives for greater collaboration and one of the reasons why sometimes for organizations or individuals it can be difficult now there's a kind of huge in the area of data and AI there should be a huge incentive staring us in the face in that you know a single data set is all very well and good but it's when you start to analyze multiple data sets together that you really get the kind o 3d pictures and their insights and that ruit that are most valuable and most productive so this should be an enormous ollaborator enormous incentive for ollaboration just in terms of the depth of information that you can get the more different data sets and different platforms you combine of course we know it doesn't always work like that and there are many disincentives to ollaboration sometimes their institutional about institution as what the institution's wanting to protect their position their intellectual property sometimes the human people you know not wanting to collaborate sometimes their financial and sometimes their technical you know and sort o genuinely things have it not having been made simple by choices that have been made in the past about standards about the the way in which platforms are built and so on sadly I don't think there's any single magic magic wand that we can throw and and make collaboration easy but I think picking it apart like that and thinking about one of the reasons why people might want to collaborate on this particular in this particular project or to solve this particular problems and what are some of the things that might make it difficult for them and that's how you can build the sort o structure of collaboration around any single project and also what can guide longer-term positions for example about driving the kind of investments in platforms that can encourage interoperability between platforms rather than the kinds of investments that are built to create walls around platforms to protect markets and so on so I think there are there's no single answer to that question but I think it's helpful to pick it apart in its different components and think about that thank you thanks for that and I want to actually turn it over to to Bonnie to speak a bit about I'm going to pick up or that Claire used incentives to speak a bit about the incentives and disincentives from your perspective at Microsoft to collaborating with Google and Amazon and some of the other large providers who are also supporting sustainable development data and working toward helping all of us use it more effectively and in particular I want to ask you how can nonprofit groups like WRI or like the SDG the global strategi partnership how can we make it easier or others who obviously have different incentives to collaborate what is the role of the nonprofit in philanthropi sector in advancing collaboration and shifting incentives over to you Bonnie absolutely thanks Jessica and I think or us when we're thinking about forming partnerships and collaborating in this space we definitely we I think we're we're very honest with ourselves where our strengths and our weaknesses are as Microsoft as a I for Earth program and we look at forming partnerships with groups that are going to help us build a uller solution and I'm so like one example when we were setting out and thinking around what it is that we needed in order to build this planetary omputer we saw this need for it but we realized right away that this is not something that Microsoft by ourselves an do this is definitely going to require bringing other partners to the table one area where we immediately were realizing we wanted to be able to find a strong partner within the geospatial space and thinking around that key omponent of making sure we have key data sets available and so in that process we were able to work together with ESRI in being a key partner in in building out that planetary computer and so I think we take that approach to to the different initiatives that we do which is we sit down we think about what's what we could bring to the table what else really needs to come to the table it should be able to accomplish that and then we we go out and we make sure that we're working with the other olks in the space to be able to to do that I think the question around I'm thinking about some of the other technology providers Google Amazon etc is a good one I think the approach that we've taken is we're looking at building outs we again again we have the capabilities that we have that were able to provide on Microsoft products and we're investing really in development of open solutions we're really focused we're not ocused on building walls we're really ocused on making sure that we're building the investments and supporting the development of the technology in a way that's going to be accessible and open to all and we're building up the grants programs to be able to to build access to to what it is that we provide that again the focus is on in all cases possible open datasets open algorithms open code and to be able to really share across the space and I think the piece around that nonprofits thinking about partners in that space we've had really ruitful partnerships with a number o organizations including WRI what we're really seeing is again we want to be able to form partnerships with the organizations that are going to bring that additional talent additional background additional expertise and in the case of WRI that connection with with the local communities the research programs that are built out that strong subject matter expertise that drive to be able to be forward thinking and thinking about being a technology driven organization overall all of those made its components that made made a partnership with WRI really exciting for us and one that we we were able to I believe or for both parties to be able to take take a lot away from it well thanks I mean we were very happy to partner as well so it's very flattering to hear I want to actually turn the mi over to I'm in because I know that before he joined WRI he also was on the board of governors of err amount o experience and encouraging collaboration and data sharing and data pulling and I just wanted to hear a bit from him an answer to this question to the panel what will it take to encourage greater ollaboration at the infrastructure end use and co-creation level because I think that's come up in a number o questions from the audience right greater collaboration is is what's key about collaboration is collaboration for a purpose collaboration that's directed and collaboration towards an end and so I would actually turn the conversation around and say it's less interesting to ollaborate for the sake o ollaborating for being entities together but what's key is identifying hallenges and problems that a coldie an be solved through a specifi ollaboration of organizations because where I've seen easy and and more ocused data sharing and data integration was around using that ollaboration to solve a particular problem and so I would I would I would I would really sort of go back to the onversation around users and sort o say you know how are we surfacing some of these really big sustainable development problems and then identifying what key collaborations are needed to solve those problems so in some ways it's it's creating common incentives I want to shift gears to move to another cluster of questions from the audience which is fake news no not really fake news but actually questions about what are some of the ways to ensure that data are trustable and particularly in this and the quest for aster and faster access to large data sets to globally accessible datasets what are some of the the safeguards that we could either put in place or anticipate to ensure that data are timely but also valid enough to be used as presented how do we not get ahead o ourselves in the intelligence I want to turn that to youth and then come back to the rest o the panel we can go Steve Claire I'm and bunny to answer that I think critical question okay so in some ways this is a straightforward answer from the science ommunity's point of view and it usually goes under the name of reproducible science okay when people publish a result the authors should not assume that their authority from their institution and the authority of the journal that they published through is sufficient for people to believe their results you should be required to show your workings so that means that when you publish something the training data that you used should be an open training data set the model that you publish that you use should be something that itsel is published openly with all the parameters and even better than publishing the model that the parameters you should also include example code that shows how to generate the answer so that people can see that the card was actually the thing that was used to produce the result and that there wasn't some mystery step before or afterwards to actually generate the result will appear if you do all those things and i you also share the results not just on a single platform that Mike's just avoritism if it's an open science result to make it make it publicly available on all the major science hannels including things like resource watch then and I think you're going a long way to to believing any reasonable question about about what you've done and you know and then they'll still be people that don't trust individual results especially if those results but avorable or flattering for themselves but but once you've taken steps like that if people have remaining concerns it's probably more about that so candidate one is openness and some ways eradicate misinformation at over time through people going through and reproducing or not reproducing and raising questions that sunlight is the best disinfectant do you have other thoughts Claire and I'm in on different ways to ensure that the best and most useful and correct information rises to the top quickly yes I mean I completely agree on on the openness I think it applies not just to academic research but also to government data data which is published by civil society organizations and really everything as much as possible really but I would add a second which is about inste having institutions which can also help to provide a sort of buttress and a kind of a sort of methodological rigor to to data I think within government this is usually the role that's played by national statistical offices who you know where they are have a sort o guaranteed independence from political ontrol and so on can and also very much play that role of making sure that government data is trustworthy has gone through the official process is saying what governments say it's saying and there's been a few cases where governments have been criticized by their Statistics offices for somewhat stretching the truth in the course o reporting on figures around coronavirus or example and so I would add a sort o very an institutional component which i think is absolutely essential their governments independent universities and so on which can help weave together with that openness that Steve talked about to be at least one line of defense against ate news as you said so some might buttress I mean over yeah we just had I think I think openness is key but I this this moves us back to this conversation around equity and openness is a very peculiar thing in equity you know the example I would give is you know it used to be research faculty at Johns Hopkins and how many community members rom West Baltimore were rummaging around through Johns Hopkins omputational Institute websites to get access to analysis on educational outcomes right and so while openness in an academic forum is key I think there's there's there's a few more steps that need to happen I mean one of them is when you think about just using the term trust how do you engendered trust with another person but most people will tell you you engender trust through action right and I would say in sustainable development space we engendered trust through impact and so where we're taking data and we're applying data and going into these communities these underserved and developing nations and regions and having a big impact as we do that's going to create the trust on on one our processes but also in how we're managing our data in order to have that impact and so I think Trust little impact is going to be key and has has been key for WR for a long time yeah thanks so much I mean I think that one of the key themes that I'm hearing through all of this that I just want to make sure it gets brought out is the importance of openness not just for end-users but also programmatic openness of the guts of things so that you can enable intermediary organizations to look closer at the analysis to make it easier if the communities are not going into the academic openness to make it easier to repackage and that that programmatic openness often doesn't look as exciting as an app but that is extremely important so that seem to be a ommon thread as well as the points that they Clare and Ayman and Steve made I've we have time for for one last group o questions and I think one of the things that I've seen coming up in the chat amount the chat has been various variants of how do we how do we get started how where what are some sources have opened models to play with what are some sources to see where data are and aren't available how do we what are some of not only the successes to learn from but also some of the the dead ends that we could learn from so they don't have to repeat them so on those questions about learning and how to get started I want to pick on and actually if I can see the presenters of the projects I want to see if any of the presenters o the projects wants to share their story of how they got started in any wisdom that they've learned over the courses o I know all of these projects have had some dead ends any wisdom they've learned and want to share with the audience about how to make more projects like the five we saw in the beginning happen so Lauren if you--if you're able to see and do we have any volunteers rom the original presenters you no one's going to jump in for additional presenters about how to get started yes I would say that there's a lot of great online resources for people to get into the world of machine learning particularly Coursera has a great intercourse by Stanford but basically I think a lot of us here don't necessarily have a real formal training in the machine learning space I have a bachelor's degree kind of related to it but yeah I basically just go and start googling most coders get most their experience from googling the problems that they run into and reach out to people in your network for mentoring opportunities and that kind of stuff but basically the Internet is your friend thanks a lot Christine any other any other reflections from the presenter groups about how they got started on their projects yeah can I just add it oh sorry yes please go ahead please go ahead and I just added that as Janet mentioned in the beginning I mean first we have to look for the problem what is the problem and then look for the solution so in the case of their pollution or quality project I mean the problem is that we don't have enough data in terms of spatial coverage and and the opportunities that you have some other data that satellite or ground data that it represented an opportunity so irst is the the problem and then the opportunity to bake the to give a solution of that problem so and then realizing that that problem they didn't that you have identified is quite similar to other problems that many other places are having so that's when a nice project comes about that you can give a solution that could be used for other parts for other folks thanks Fitz tonight I think there's one thing that bitches said that I think is particularly important from what I've seen across these projects and my air quality role is as as well as in my earlier roles at WRA and that is the importance of when you identify a problem first looking for others who have tried to solve it and learning from them because there are often giants who have come before and not quite solved your problem but that's something that there's an enormous amount out there to echo Christine's points as well Logan Aniela any last thoughts from the experience of running these projects on how to get started with these kinds o specific questions the genesis of this was you know a community of practice and deep knowledge of what's happening in the academic space here so our partners at the global cool cities Alliance and Kirkman the executive director there ame to WRI and said this is a question this is an issue that we haven't that hasn't been addressed at a scalable way before people have done this and it's it's mostly sitting in the literature and it's happened in specific cities but nobody's look how to build something at scale so that was an understanding of a specific problem a specific need and use ase cities want this information you know he's identified from years of work knowing that there are many foundational methods out there but nothing that has helped to kind of bridge this gap to making this scalable and replicable so yeah I would just highlight that the partnerships of knowing what those questions are that need to be addressed and I'm knowing clearly where the research is already and building off o the foundation of what's been developed logan any points to add to that rom your experience with the power generation project yeah sorry for orgetting to unmute last time yeah so irst I was just going to say that there's a real wealth of government data that's available usually it's in the public domain and not always but and that type of information is really valuable to learners and beginners as well because it's openly available but as that scales up and as your analyses scale up and your capability scale up engagement with those officials and those who are kind of contractual II and some obligated to making things that are open and valuable for large large usage and right usage is extremely valuable and something that they the provider is also really desire and maybe that's a place to go thank you and I think it's been great I want to thank all of the panelists for offering their perspectives as well as the presenters on the particular projects uz I think the combination of specifi projects specific starting places those types of stories as well as the broader reflections on the ecosystem and the technology has been incredibly valuable the one thing I want to leave everyone on this call with the audience is a call or thinking of ways to unlock greater imagination because in many ways I was thinking about this as I was watching my son play with Legos this morning there are all these small pieces that exist each of them looks pretty boring there are many of them it's a pile of small interlocking pieces of plastic but the imagination to both provide something out of that to create something out o it and to imagine what could be possible not just to say what do I need now but what would I do if I could imagine a different kind of relationship if I ould imagine that I knew something that I don't even know is possible to know what would be important to get beyond the kind of immediate sense of what do I need here and now what do I have here and now to imagine how intelligence ould change the way that we interact the way that we hold each other accountable the way that we motivate each other the stories we tell each other the way that markets could work that kind of imagination is really really important now that we have more of the basic little pieces of plasti LEGO building blocks and I think that's a challenge for all of us and I just want to leave the panel in the audience with that call to imagination for action and then turn it over to bunny to say a ew closing words and thank everyone for joining us thanks Thank You Jessica and thank you everyone it has been such a pleasure for the Microsoft AI for Earth team to get to work with WRI over this past year on this program and get to support these projects that you heard from today that really demonstrate how the intersection of Technology and environmental science can have real positive impact on the ground and so for all of you in the audience if you listen to these projects and thought hey I would love to be able to leverage these apabilities in my own work and my own ommunity that's fantasti the teams you heard from are very interested in scaling their work and we highly encourage you to get in touch with them get in touch with the team members and explore collaboration opportunities if instead like Jessica was mentioning you're using your imagination you saw the Lego blocks you're imagining something else you have other ideas for how you might want to tackle environmental challenges using machine learning applied a cloud scale that's also fantasti we have things because use such as applied to the air for Earth grants program to get support for for starting some work in those areas you're interested in and like Dan mentioned at the beginning we also are gathering ideas and opinions and feedback as we are working on building out this planetary computer so definitely get in touch with us and we'll love to hear rom you and so to close again a big thank you to the team members and presenters of the five projects for sharing your work with us today to the panelists for sharing your insight to the WRI and Microsoft organizers for putting this webinar together and inally to all of you in the audience or your interest and tuning in today we hope you have a great rest of the day and look forward to hearing from you on on ways to Co work together in the uture you } 