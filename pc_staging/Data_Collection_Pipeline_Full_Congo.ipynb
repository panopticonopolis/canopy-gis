{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel Hub Config\n",
    "\n",
    "from env_vars import sentinel_hub_instance_id\n",
    "from sentinelhub import SHConfig\n",
    "\n",
    "# Import Area of Interest List\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from scripts.mgrs import encode,LLtoUTM\n",
    "\n",
    "\n",
    "# Sentinel Hub Tile Look Up / Download\n",
    "\n",
    "from sentinelhub import WebFeatureService, BBox, CRS, DataSource, AwsTileRequest\n",
    "\n",
    "\n",
    "# Cloud Masking\n",
    "\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import earthpy.mask as em\n",
    "\n",
    "# Generate Product Detail DataFrame\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "# Sort / Organize Tiles by Individual Folders\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "# Reproject Masked Files \n",
    "\n",
    "import gdal\n",
    "from glob import glob\n",
    "\n",
    "# Create Master Raster\n",
    "\n",
    "\n",
    "# Extract Polygon crops from products\n",
    "\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import earthpy.spatial as es\n",
    "\n",
    "# TIF to JPG\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdal.UseExceptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trailing_slash(path):\n",
    "    if path[-1] != '/':\n",
    "        path += '/'\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(output_dir):\n",
    "    # If the output folder doesn't exist, create it\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shub_connect(sentinel_hub_instance_id):\n",
    "\n",
    "    INSTANCE_ID = sentinel_hub_instance_id  \n",
    "\n",
    "    if INSTANCE_ID:\n",
    "        config = SHConfig()\n",
    "        config.instance_id = INSTANCE_ID\n",
    "    else:\n",
    "        config = None\n",
    "        \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = shub_connect(sentinel_hub_instance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_aois(csv_loc):    \n",
    "\n",
    "    df_labels = pd.read_csv(csv_loc)\n",
    "    df_labels = df_labels[[\"center-lat\",\"center-long\",\"polygon\"]][0:33]\n",
    "\n",
    "    polygons = []\n",
    "    for polygon in df_labels[\"polygon\"]:\n",
    "        polygons.append(json.loads(polygon)[\"coordinates\"])\n",
    "\n",
    "    coordinates = []\n",
    "    for items in polygons:\n",
    "        for item in items:\n",
    "            for lon_lat in item:\n",
    "                coordinates.append(lon_lat)\n",
    "\n",
    "    #bounding box\n",
    "\n",
    "    min_lon = min([i[0] for i in coordinates])\n",
    "    min_lat = min([i[1] for i in coordinates])\n",
    "    max_lon = max([i[0] for i in coordinates])\n",
    "    max_lat = max([i[1] for i in coordinates])\n",
    "\n",
    "    bounding_box = min_lon,min_lat,max_lon,max_lat\n",
    "\n",
    "\n",
    "    tiles = []\n",
    "    for ll in coordinates:\n",
    "        tiles.append(encode(LLtoUTM(ll[1],ll[0]),1)[:-2])\n",
    "\n",
    "    tiles = list(set(tiles))\n",
    "    return bounding_box,tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box,tile_list = import_aois(\"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/AOI/labels_Misha_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519 289\n",
      "(5.449219, -5.703448, 31.376953, 8.711359) ['34NBH', '31MGS', '32NLF', '36NTF', '33NSF', '34NFG', '35NNG', '34MHV', '34NAJ', '35NPJ', '32NNN', '33MSS', '33MWV', '35MLS', '36MSE', '36NSN', '32MQD', '33NXA', '35NNH', '32MKV', '32MRE', '33NTF', '32MKE', '33NTH', '35NMC', '34MDU', '35NRH', '36MSU', '32MRD', '34MGA', '32NLN', '32MMV', '32MNV', '35MMR', '33MXS', '33NUB', '35NND', '34PCQ', '35MNQ', '33PUJ', '32NJM', '35MNS', '35MPU', '32MMC', '34PDQ', '33NVG', '33NUC', '32MQA', '36NSJ', '36NTG', '31NGE', '34MGC', '34NEK', '31PHK', '32PPQ', '33MTR', '33MWQ', '32NNL', '34MAU', '36NTK', '34NGM', '32NPN', '33NSB', '33NVJ', '32NLG', '33NTB', '32NJH', '31MHS', '33NWJ', '34MAB', '32MJE', '33MVS', '35PJJ', '35NJH', '31MGQ', '32NNH', '36MSC', '33NXH', '32NRG', '33MUQ', '31NHE', '35NPH', '34NHN', '35NMB', '36PSP', '33MTU', '35NKC', '32PKP', '34NCH', '35MRV', '31MHR', '35MLQ', '32MJC', '33PSK', '33NXC', '33NUF', '34MCV', '34NFF', '33PZJ', '35NLG', '31MGP', '35MRQ', '35MLU', '34MFC', '32MRU', '31NHH', '33MZU', '34MEV', '31NHF', '33NXF', '33NVE', '34MCD', '34MFD', '33NWD', '35NRC', '34NAM', '33NWA', '31MHU', '33NSH', '32PMP', '36NSM', '35PRJ', '33MTT', '34NEJ', '33PWJ', '31NHC', '33MVV', '34MHB', '33PZK', '35MQP', '32MRB', '34NBG', '34PGP', '34MDA', '35PJK', '36NSH', '36NTH', '33NYB', '32NLM', '32MNA', '33MSU', '35NJF', '31NHA', '33NUJ', '35NMG', '33NZB', '34MGU', '33NVA', '34NDF', '34MFA', '32NMF', '35NNB', '33MTV', '32PNP', '33PTJ', '34MBC', '33PVJ', '33NWH', '32PJQ', '35MNU', '36MTU', '34NCL', '35NPB', '31NGH', '32NNJ', '36NUG', '35NJA', '35NMD', '33MZQ', '33MSR', '32MJU', '33MTS', '33NYH', '32MMA', '34NHP', '35NJG', '35PMK', '35NKJ', '36MSA', '36MSV', '31MGR', '33NSA', '35NME', '32PRQ', '35NPF', '34NFP', '35PPK', '34NBL', '35MNP', '32NJN', '32NLL', '34MBB', '32NML', '33NTG', '35MJS', '33MWS', '33MUT', '33NVF', '35NJJ', '33NVH', '34PHQ', '34NAF', '36MTA', '34NHF', '32PLQ', '35NKH', '35NPA', '35MRP', '36MTV', '34NCP', '32MJB', '33MYV', '32NNG', '32PNQ', '34NEH', '35MNV', '33NWG', '33NZG', '36NUF', '33MWP', '34MDB', '32MQV', '35PKK', '33MUU', '34NDH', '31MGT', '36NSL', '36NUM', '33MSP', '35NQE', '33PYK', '36MTB', '33NZH', '36NUK', '33NSC', '34NHK', '34NAK', '33PXK', '34MEA', '34MAD', '34NGJ', '35NMH', '35MMQ', '32MRA', '33PTK', '35MRR', '35PQK', '32NKM', '34NGG', '35MPV', '32NKL', '32MRV', '32NPF', '32NPL', '33NSJ', '33MVQ', '33NZE', '34NDJ', '35MJV', '35NRD', '32NKG', '35PPJ', '35NLE', '35MMS', '31NHJ', '32MMD', '32NRN', '35PRK', '34NAL', '34NAN', '32PQP', '35NRA', '36MUE', '35NPE', '34MEC', '32PLP', '32NMH', '34PAQ', '32NLH', '35MQU', '34NFL', '35NNE', '32NPM', '34MHC', '32NNF', '35NJD', '36MUA', '34PHP', '35NQA', '33NZF', '36NSF', '35MQQ', '32NRL', '35MPP', '33MZV', '36NSG', '32PMQ', '35MJU', '32NMK', '32NKP', '32NLJ', '32NQJ', '33NYG', '35MKQ', '35NNJ', '33NSG', '33MXU', '32PRP', '34NEF', '32NRJ', '31NHD', '33NTJ', '32PQQ', '35MMU', '36NTL', '34MHA', '35NQB', '34MBA', '32NNM', '33MYS', '36MUC', '32NKJ', '33NXD', '32NMM', '35NKG', '36PTQ', '32MLC', '32NMP', '33MYT', '31MGV', '34NDM', '34MGV', '32NNP', '32NMJ', '34NBP', '32NQK', '35NQJ', '36NUJ', '32MNE', '35NJE', '35NLD', '35NJC', '35NMF', '32NJL', '36NTP', '35NKB', '33NWF', '34NCG', '36MSB', '31MHV', '33NSD', '35MJP', '32NRM', '32NKK', '35MMT', '33NWB', '33MUV', '31NGB', '35NRF', '36NTM', '34NBF', '32MKB', '35NMJ', '34NHM', '34PEP', '33MVT', '34NGL', '31NGF', '32MPD', '33NYE', '32MLV', '33NYJ', '33PSJ', '34PAP', '34NHL', '34MCU', '33PYJ', '34MFE', '32NJP', '32NPH', '35NNA', '33MTP', '34MDD', '34NCF', '33PVK', '33NXG', '33MYP', '34MHU', '34PDP', '34MHE', '35NQC', '35PLK', '33MUP', '33MZP', '34NCK', '32MPC', '35MPT', '36MSD', '33NTC', '32NQM', '32NRF', '33MUS', '32NQL', '34NEG', '35MKT', '35MQV', '33MWR', '34PBQ', '35NQH', '33MTQ', '34NDK', '33NUE', '32MPV', '36PTP', '34NBN', '35MPR', '33NVC', '32MQC', '31PHJ', '32MKD', '35NLH', '32MLA', '32MNB', '32MNC', '33NXE', '33MXV', '33MST', '35NLB', '34NEN', '33MZR', '35MRT', '34MFU', '35NPC', '32MJV', '33MXQ', '32NQP', '33PWK', '32NJK', '34PFQ', '33NYA', '34PEQ', '34MCE', '33MVU', '35MNT', '32NRK', '34NBK', '34NGK', '34MFB', '33MXR', '34MCB', '35NPD', '34MHD', '32MMB', '34NHJ', '34MAC', '35NNF', '34MDC', '31PGJ', '34NCJ', '35NMA', '34NBM', '36NTJ', '35MJR', '31MHQ', '34NGN', '33NZC', '35MPS', '33PXJ', '33MUR', '36MUV', '36NSP', '35MJT', '35NKD', '32NJF', '35NKA', '31PGK', '35PMJ', '35NQF', '32PJP', '32NRP', '36MTE', '35MPQ', '32MJA', '32NKH', '33NUA', '35MLT', '32PKQ', '36MUD', '34MFV', '33MYQ', '34MDE', '35MLP', '33NVB', '34MAV', '34NHH', '32NLK', '31NGJ', '32NPP', '32MLD', '35NKF', '34NAG', '36NUL', '32MQB', '34PGQ', '36PUQ', '31NGG', '32NMN', '34NDN', '31NGA', '34NEP', '33NSE', '33NZJ', '32MPB', '32MQE', '34MCC', '35NNC', '34MCA', '35NLF', '35PLJ', '35MQT', '36NUH', '35MRU', '35MLV', '32PPP', '31NHB', '32NMG', '36NTN', '34MAA', '31NGC', '32MLU', '35NJB', '33NWE', '32NPJ', '34MDV', '32MPE', '34NEM', '32MQU', '34MBV', '35NRE', '32NQH', '32MLE', '35PNJ', '31MHT', '33NZD', '31MHP', '34MED', '35NLJ', '35PQJ', '32NQF', '34NCN', '34MBE', '32NQN', '34PFP', '33NYF', '36NUP', '32NKN', '32MNU', '33NUH', '33MYU', '33MXP', '34NFJ', '34MEU', '35NRB', '35MKP', '32MND', '35MMV', '34PCP', '35MKV', '35NPG', '33NVD', '35MJQ', '36MUB', '36NUN', '32NJG', '35NQD', '35NLA', '33MSV', '33NTA', '34NGH', '35NKE', '36PSQ', '32NLP', '35MQR', '33MVP', '34MAE', '35MMP', '32NRH', '35PNK', '34MGD', '33NTE', '32MKU', '31MGU', '32MKA', '33NYC', '35MQS', '32MPU', '34NHG', '34NAP', '35MNR', '35MLR', '33MVR', '36MTD', '34NGF', '34NFM', '34NEL', '34MBU', '34NFH', '32MPA', '34MGE', '34NGP', '35PKJ', '33NYD', '33MZS', '34MGB', '34MEE', '32MLB', '34NDG', '33NTD', '34PBP', '36MUU', '33MZT', '32NQG', '32NPG', '35NRG', '34NAH', '33MWU', '35MKU', '32MMU', '32NNK', '33NZA', '34NDL', '35NRJ', '33MWT', '34NDP', '36PUP', '33MYR', '34MBD', '35MRS', '33NWC', '34MEB', '32MME', '33PUK', '31NHG', '33MSQ', '35NQG', '33NUD', '36NSK', '33NXJ', '35MKS', '33NUG', '33NXB', '32MRC', '32NPK', '34NBJ', '32NKF', '35MKR', '32NJJ', '34NFN', '32MJD', '34NFK', '31NGD', '34NCM', '36MTC', '33MXT', '32MKC', '35NLC']\n"
     ]
    }
   ],
   "source": [
    "polygon = {\"type\":\"Polygon\",\"coordinates\":[[[5.493164,8.276727],[5.449219,-5.703448],[31.376953,-4.959615],[31.157227,8.711359],[5.493164,8.276727]]]}\n",
    "\n",
    "items = json.loads(str(polygon[\"coordinates\"]))\n",
    "coordinates = []\n",
    "for item in items:\n",
    "            for lon_lat in item:\n",
    "                coordinates.append(lon_lat)\n",
    "#                 print(coordinates)\n",
    "# tiles = []\n",
    "# for ll in coordinates:\n",
    "#     tiles.append(encode(LLtoUTM(ll[1],ll[0]),1)[:-2])\n",
    "# #     print(ll[1],ll[0])\n",
    "\n",
    "\n",
    "#bounding box\n",
    "\n",
    "min_lon = min([i[0] for i in coordinates])\n",
    "min_lat = min([i[1] for i in coordinates])\n",
    "max_lon = max([i[0] for i in coordinates])\n",
    "max_lat = max([i[1] for i in coordinates])\n",
    "\n",
    "lon_list = []\n",
    "bounding_box = min_lon,min_lat,max_lon,max_lat\n",
    "\n",
    "for lon in np.arange(min_lon,max_lon,.05):\n",
    "    lon_list.append(lon)\n",
    "    \n",
    "lat_list = []\n",
    "for lat in np.arange(min_lat,max_lat,.05):\n",
    "    lat_list.append(lat)\n",
    "\n",
    "print(len(lon_list),len(lat_list))\n",
    "\n",
    "\n",
    "lon_lat = []\n",
    "for lon_2 in lon_list:\n",
    "    for lat_2 in lat_list:\n",
    "        lon_lat.append([lon_2,lat_2])\n",
    "\n",
    "tiles_2 = []\n",
    "for ll_2 in lon_lat:\n",
    "    tiles.append(encode(LLtoUTM(ll_2[1],ll_2[0]),1)[:-2])\n",
    "\n",
    "\n",
    "\n",
    "tile_list = list(set(tiles))\n",
    "print(bounding_box,tile_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tile_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shub_lookup_tiles(bounding_box,tile_list,search_time_interval = ('2019-01-01T00:00:00', '2020-12-31T23:59:59'),\n",
    "                   product_type = DataSource.SENTINEL2_L2A):\n",
    "    \n",
    "    #Misha's Tiles of Interest\n",
    "    search_bbox = BBox(bbox=bounding_box, crs=CRS.WGS84)\n",
    "\n",
    "    search_time_interval = ('2019-01-01T00:00:00', '2020-12-31T23:59:59')\n",
    "    wfs_iterator = WebFeatureService(\n",
    "        search_bbox,\n",
    "        search_time_interval,\n",
    "        data_source=product_type,\n",
    "        maxcc=.05,\n",
    "        config=config\n",
    "    )\n",
    "    results = wfs_iterator.get_tiles()\n",
    "    df = pd.DataFrame(results, columns=['Tilename','Date','AmazonID'])\n",
    "    df_tiles_of_interest = df[df[\"Tilename\"].isin(tile_list)]\n",
    "    df2 = df_tiles_of_interest.groupby('Tilename').head(10)\n",
    "    output2 = list(df2.itertuples(index=False,name=None))\n",
    "    return df,df2,output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,df2,output2 = shub_lookup_tiles(bounding_box,tile_list,search_time_interval = ('2019-01-01T00:00:00', '2020-12-31T23:59:59'),\n",
    "                   product_type = DataSource.SENTINEL2_L2A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35PNK    101\n",
       "35PLK     96\n",
       "35PPK     93\n",
       "36MTV     93\n",
       "35NNJ     88\n",
       "        ... \n",
       "33NTC      1\n",
       "32NPF      1\n",
       "32MNE      1\n",
       "32NPJ      1\n",
       "33NTD      1\n",
       "Name: Tilename, Length: 515, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Tilename\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35NMJ    10\n",
       "35NNC    10\n",
       "33NZE    10\n",
       "35NPD    10\n",
       "34NEJ    10\n",
       "         ..\n",
       "32MNE     1\n",
       "33NTC     1\n",
       "32MQE     1\n",
       "32NPF     1\n",
       "32NPJ     1\n",
       "Name: Tilename, Length: 515, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"Tilename\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shub_lookup_tiles(bounding_box,tile_list,search_time_interval = ('2019-01-01T00:00:00', '2020-12-31T23:59:59'),\n",
    "                   product_type = DataSource.SENTINEL2_L2A):\n",
    "    \n",
    "    #Misha's Tiles of Interest\n",
    "    search_bbox = BBox(bbox=bounding_box, crs=CRS.WGS84)\n",
    "\n",
    "    search_time_interval = ('2019-01-01T00:00:00', '2020-12-31T23:59:59')\n",
    "    wfs_iterator = WebFeatureService(\n",
    "        search_bbox,\n",
    "        search_time_interval,\n",
    "        data_source=product_type,\n",
    "        maxcc=.05,\n",
    "        config=config\n",
    "    )\n",
    "    results = wfs_iterator.get_tiles()\n",
    "    df = pd.DataFrame(results, columns=['Tilename','Date','AmazonID'])\n",
    "    df_tiles_of_interest = df[df[\"Tilename\"].isin(tile_list)]\n",
    "    df2 = df_tiles_of_interest.groupby('Tilename').head(10)\n",
    "    output2 = list(df2.itertuples(index=False,name=None))\n",
    "    return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = shub_lookup_tiles(bounding_box,tile_list,search_time_interval = ('2019-01-01T00:00:00', '2020-12-31T23:59:59'),\n",
    "                   product_type = DataSource.SENTINEL2_L2A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shub_download_tiles(results_list,output_dir,bands=[\"R10m/TCI\"],product_type = DataSource.SENTINEL2_L2A):\n",
    "    \n",
    "    #Additional Params\n",
    "    bands = bands\n",
    "    \n",
    "    output_dir = add_trailing_slash(output_dir)\n",
    "    \n",
    "    \n",
    "    for tile in results_list:\n",
    "        tile_name, time, aws_index = tile\n",
    "\n",
    "        #Download SAFE Files\n",
    "        request = AwsTileRequest(\n",
    "            tile=tile_name,\n",
    "            time=time,\n",
    "            bands = bands, \n",
    "            aws_index=aws_index,\n",
    "            data_folder=output_dir,\n",
    "            data_source=product_type,\n",
    "            safe_format = True\n",
    "        )\n",
    "\n",
    "        request.save_data(redownload=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test\"\n",
    "\n",
    "shub_download_tiles(results_list,output_dir,bands=[\"R10m/TCI\"],product_type = DataSource.SENTINEL2_L2A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud_mask_tci(prod_dir):\n",
    "    \n",
    "    ''''\n",
    "    \n",
    "    prod refers product directory \n",
    "    \n",
    "    ''''\n",
    "    \n",
    "    prod_dir = add_trailing_slash(prod_dir)\n",
    "    \n",
    "    msk_file_path = glob(src_dir + \"*/*/MSK_CLDPRB_20m.jp2\")[0]\n",
    "    tci_file_path = glob(src_dir + \"*/IMG_DATA/R10m/*.jp2\")[0]\n",
    "    tci_filename = tci_file_path.split(\"/\")[-1]\n",
    "    output_tci_file_path = src_dir + \"/IMG_DATA/R10m/\" + \"processed_\" + tci_filename \n",
    "\n",
    "    nodatavalue = int(0)\n",
    "\n",
    "    with rio.open(tci_file_path) as sen_TCI_src:\n",
    "        sen_TCI = sen_TCI_src.read(masked=True)\n",
    "        sen_TCI_meta = sen_TCI_src.meta\n",
    "\n",
    "    with rio.open(msk_file_path) as sen_mask_src:\n",
    "        sen_mask_pre = sen_mask_src.read(1)\n",
    "        sen_mask = np.repeat(np.repeat(sen_mask_pre,2,axis=0),2,axis=1)\n",
    "\n",
    "    # All pixels above 0 probability will be classified as True\n",
    "\n",
    "    sen_mask_qa = sen_mask > 0\n",
    "\n",
    "\n",
    "    # Apply mask to source TCI file\n",
    "    if np.count_nonzero(sen_mask_qa) > 0:\n",
    "        sen_TCI_cl_free_nan = em.mask_pixels(sen_TCI, sen_mask_qa)\n",
    "        sen_TCI_cl_free_processed = np.ma.filled(sen_TCI_cl_free_nan, fill_value=nodatavalue)\n",
    "    else:\n",
    "        sen_TCI_c1_free_processed = sen_mask_qa\n",
    "\n",
    "\n",
    "    # Export cloud-masked TCI file\n",
    "    with rio.open(output_tci_file_path, 'w', **sen_TCI_meta) as outf:\n",
    "        outf.write(sen_TCI_cl_free_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask_tci_safe_list(products_dir):\n",
    "    ''''\n",
    "    \n",
    "    products_dir refers to parent directory containing multiple products\n",
    "    \n",
    "    \n",
    "    ''''\n",
    "    \n",
    "    products_dir = add_trailing_slash(products_dir)\n",
    "    \n",
    "    dir_list = glob(products_dir + \"/*\" )\n",
    "    \n",
    "    \n",
    "    for directory in dir_list:\n",
    "        cloud_mask_tci(directory)\n",
    "        \n",
    "    print(f\"Applied masks to {len(dir_list)} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied masks to 4 products\n"
     ]
    }
   ],
   "source": [
    "tci_folder_list = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_raw\"\n",
    "apply_mask_tci_safe_list(tci_folder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_product_detail_df(input_dir):\n",
    "    \n",
    "    '''\n",
    "    Generate product details dataframe used as input for ordering products by Cloudy Pixel Percentage, No Data Pixel Percentage, or Unclassified Percentage\n",
    "    \n",
    "    '''\n",
    "    input_dir = add_trailing_slash(input_dir)\n",
    "    \n",
    "    dirs = os.listdir(input_dir)\n",
    "\n",
    "    meta_data = []\n",
    "    for folder in dirs:\n",
    "        xml_loc = glob(input_dir + \"/\" + folder + \"/*.xml\")[0]\n",
    "        tree = ET.parse(xml_loc)\n",
    "        directory = [elem.text for elem in tree.iter() if \"MASK_FILENAME\" in elem.tag][0].split(\"/\")[1]\n",
    "        tile_id = directory.split(\"_\")[1]\n",
    "        filepath_partial = input_dir + \"/\" + directory + \"/IMG_DATA\" + \"/R10m\"\n",
    "        filepath = glob(filepath_partial + \"/processed*.jp2\")[0]\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        cloud_cover,no_data,unclassified = [elem.text for elem in tree.iter() if \"CLOUDY_PIXEL_PERCENTAGE\" in elem.tag \n",
    "                 or \"NODATA_PIXEL_PERCENTAGE\" in elem.tag or \"UNCLASSIFIED_PERCENTAGE\" in elem.tag]\n",
    "        meta_data.append([directory,tile_id,cloud_cover,no_data,unclassified,filename,filepath])\n",
    "    df = pd.DataFrame(meta_data,columns=[\"Directory\",\"Tile_Id\",\"Cloud Cover\",\"No Data Percentage\",\"Unclassified Percentage\",\"Filename\",\"Filepath\"])\n",
    "    df2 = df.sort_values(by=[\"Tile_Id\",\"Cloud Cover\",\"Unclassified Percentage\"],ignore_index=True)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_raw\"\n",
    "\n",
    "df = generate_product_detail_df(input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_masked_tiles(df,output_dir):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df input is the products detail pre-sorted dataframe to be used for sorting products \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    output_dir = add_trailing_slash(output_dir)\n",
    "\n",
    "    layer = 1\n",
    "    for index,row in df.iterrows(): \n",
    "        destination_dir = output_dir + str(layer)\n",
    "        output_file = destination_dir + \"/\" + row[\"Filename\"]\n",
    "\n",
    "        # Check if directory exists\n",
    "        if not os.path.isdir(destination_dir):\n",
    "            os.mkdir(destination_dir)\n",
    "\n",
    "        # Copy file to existing or new directory\n",
    "        copyfile(row[\"Filepath\"],output_file)\n",
    "\n",
    "        # Check if Tile_Id already exists in the directory - only necessary up until the last tile\n",
    "        if len(df) > index + 1:\n",
    "            if df.loc[index,\"Tile_Id\"] == df.loc[index + 1,\"Tile_Id\"]:\n",
    "                layer += 1\n",
    "            else:\n",
    "                layer = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_masked_tiles(df,\"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_gdf(csv_loc):\n",
    "    '''\n",
    "    import manually created areas of interest csv\n",
    "    \n",
    "    output is an in-memory geo dataframe with one polygon AOI per row to be utilized for cropping master raster\n",
    "    \n",
    "    '''\n",
    "    df = pd.read_csv(csv_loc)\n",
    "    df_labels = df[[\"center-lat\",\"center-long\",\"polygon\"]][0:33]\n",
    "\n",
    "\n",
    "    polygons = []\n",
    "    for polygon in df_labels[\"polygon\"]:\n",
    "        polygon_temp = []\n",
    "        for coordinates in json.loads(polygon)[\"coordinates\"]:\n",
    "            for coordinate in coordinates:\n",
    "                polygon_temp.append(tuple(coordinate))\n",
    "            polygons.append(Polygon(polygon_temp))\n",
    "\n",
    "    gdf_series = gpd.GeoSeries(polygons)\n",
    "    gdf = gpd.GeoDataFrame(gdf_series,geometry=0)\n",
    "    gdf[\"geometry\"] = gdf[0]\n",
    "    gdf = gdf.drop(columns=[0])\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_loc = \"/Users/purgatorid/Documents/GitHub/canopy-gis/data_collection/data/labelled/labels_Misha_v2.csv\"\n",
    "\n",
    "gdf = csv_to_gdf(csv_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rasters(src_dir, dest_dir, epsg_format='EPSG:4326', windows=False):\n",
    "    \"\"\"Converts the rasters in the src_dir into a different EPSG format,\n",
    "    keeping the same folder structure and saving them in the dest_dir.\"\"\"\n",
    "\n",
    "    src_dir = add_trailing_slash(src_dir)\n",
    "    dest_dir = add_trailing_slash(dest_dir)\n",
    "    \n",
    "    # If the output folder doesn't exist, create it\n",
    "    create_dir(dest_dir)\n",
    "\n",
    "    input_files = glob(src_dir + '*/*.jp2')\n",
    "    # Keep track of how many files were converted\n",
    "    n = 1\n",
    "    total = len(input_files)\n",
    "    \n",
    "    for f in input_files:\n",
    "        print(f'processing file {n} of {total}')\n",
    "        n += 1\n",
    "        \n",
    "        # The way we've set it up, we save each product into a numbered folder,\n",
    "        # depending on which layer it's in. To keep this structure, we need to\n",
    "        # pull out the folder number from the file path.\n",
    "        # How exactly to do this depends on if you're using Windows or not,\n",
    "        # since the path conventions are different.\n",
    "        if windows:\n",
    "            folder_num = f.split('\\\\')[-2]\n",
    "            filename = f.split('\\\\')[-1]\n",
    "        else:\n",
    "            folder_num = f.split('/')[-2]\n",
    "            filename = f.split('/')[-1]\n",
    "        output_folder = dest_dir + folder_num + '/'\n",
    "        \n",
    "        \n",
    "        # If the respective grouping folders are not available \n",
    "        create_dir(output_folder)\n",
    "        \n",
    "        output_filepath = output_folder + filename\n",
    "        \n",
    "        print(output_filepath)\n",
    "        print(f)\n",
    "\n",
    "        # Finally, we convert\n",
    "        converted = gdal.Warp(output_filepath, [f],format='GTiff',\n",
    "                              dstSRS=epsg_format, resampleAlg='near')\n",
    "        converted = None\n",
    "        \n",
    "    print('Finished')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file 1 of 4\n",
      "/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered_warped/1/processed_T33NYC_20200927T090731_TCI_10m.jp2\n",
      "/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered/1/processed_T33NYC_20200927T090731_TCI_10m.jp2\n",
      "processing file 2 of 4\n",
      "/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered_warped/1/processed_T34MBB_20200926T084719_TCI_10m.jp2\n",
      "/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered/1/processed_T34MBB_20200926T084719_TCI_10m.jp2\n",
      "processing file 3 of 4\n",
      "/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered_warped/1/processed_T34NEF_20200906T084559_TCI_10m.jp2\n",
      "/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered/1/processed_T34NEF_20200906T084559_TCI_10m.jp2\n",
      "processing file 4 of 4\n",
      "/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered_warped/2/processed_T33NYC_20200922T090659_TCI_10m.jp2\n",
      "/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered/2/processed_T33NYC_20200922T090659_TCI_10m.jp2\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "src_dir = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered\"\n",
    "dest_dir = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered_warped\"\n",
    "\n",
    "convert_rasters(src_dir, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_virtual_raster(src_dir, dest_dir, num_layers=2):\n",
    "    \"\"\"Combines the rasters in the src_dir into a single virtual raster\n",
    "    with proper prioritization. This is saved into the dest_dir.\n",
    "    Make sure the num_layers variable is the same as the number of tile layers\n",
    "    in your src_dir.\"\"\"\n",
    "    \n",
    "    src_dir = add_trailing_slash(src_dir)\n",
    "    dest_dir = add_trailing_slash(dest_dir)\n",
    "    \n",
    "    # If the output folder doesn't exist, create it\n",
    "    create_dir(dest_dir)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for layer in range(1, num_layers+1):\n",
    "        print('Making Layer', layer)\n",
    "        \n",
    "        # Get the filenames from the layer in question\n",
    "        filenames = glob(src_dir + f'{layer}/*.jp2', recursive=True)\n",
    "        \n",
    "        output_file = dest_dir + f'Layer{layer}.vrt'\n",
    "    \n",
    "        vrt = gdal.BuildVRT(output_file, filenames, resolution='average', resampleAlg='nearest', srcNodata=0)\n",
    "    \n",
    "        vrt.FlushCache()\n",
    "    \n",
    "    print('Making full raster')\n",
    "\n",
    "    # To make the full raster, we combine every layer. Do it in reverse order because (I believe)\n",
    "    # the last items in the list are prioritized.\n",
    "\n",
    "    input_files = [dest_dir + f'Layer{i}.vrt' for i in reversed(range(1, num_layers+1))]\n",
    "    \n",
    "    output_file = dest_dir + 'full.vrt'\n",
    "\n",
    "    vrt = gdal.BuildVRT(output_file, input_files, resolution='average', resampleAlg='nearest', srcNodata=0)\n",
    "\n",
    "    vrt.FlushCache()\n",
    "\n",
    "    print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Layer 1\n",
      "Making Layer 2\n",
      "Making full raster\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "src_dir = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered_warped\"\n",
    "dest_dir = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_master_raster\"\n",
    "\n",
    "make_full_virtual_raster(src_dir, dest_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vrt_to_tif(output_file,src_file):\n",
    "\n",
    "    translate = gdal.Translate(\"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_master_raster/full_tif.tif\", \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_master_raster/full.vrt\",\n",
    "                               format='GTiff')\n",
    "    translate.FlushCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_aoi_polygon_rasters(gdf,master_raster_path,output_dir):\n",
    "    \n",
    "    output_dir = add_trailing_slash(output_dir) \n",
    "\n",
    "    src_raster_file = rio.open(master_raster_path)\n",
    "    \n",
    "    for index in range(gdf.shape[0]):\n",
    "        crop_extent = gdf.loc[[index],\"geometry\"]\n",
    "\n",
    "\n",
    "        raster_crop, raster_meta = es.crop_image(src_raster_file, crop_extent)\n",
    "\n",
    "        # Update the metadata to have the new shape (x and y and affine information)\n",
    "        raster_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": raster_crop.shape[1],\n",
    "                         \"width\": raster_crop.shape[2],\n",
    "                         \"transform\": raster_meta[\"transform\"]})\n",
    "\n",
    "        # generate an extent for the newly cropped object for plotting\n",
    "        cr_ext = rio.transform.array_bounds(raster_meta['height'], \n",
    "                                                    raster_meta['width'], \n",
    "                                                    raster_meta['transform'])\n",
    "        \n",
    "        \n",
    "\n",
    "        bound_order = [0,2,1,3]\n",
    "        cr_extent = [cr_ext[b] for b in bound_order]\n",
    "\n",
    "        # mask the nodata\n",
    "        raster_crop_ma = np.ma.masked_equal(raster_crop, 0) \n",
    "\n",
    "\n",
    "        # output_path\n",
    "        outpath = out_base_path + str(index+1) + '.tif'\n",
    "\n",
    "        # Check if directory exists\n",
    "        if not os.path.isdir(out_base_path):\n",
    "            os.mkdir(out_base_path)\n",
    "\n",
    "\n",
    "        # Export cloud-masked TCI file\n",
    "        with rio.open(outpath, 'w', **raster_meta) as outf:\n",
    "            outf.write(raster_crop_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b712d91838e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/Polygon_Crops_Test/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexport_aoi_polygon_rasters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaster_raster_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-43e5cf1db1df>\u001b[0m in \u001b[0;36mexport_aoi_polygon_rasters\u001b[0;34m(gdf, master_raster_path, output_dir)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mraster_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraster_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_raster_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_extent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Update the metadata to have the new shape (x and y and affine information)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/data_collection-Bxb0R3Gi/lib/python3.7/site-packages/earthpy/spatial.py\u001b[0m in \u001b[0;36mcrop_image\u001b[0;34m(raster, geoms, all_touched)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mclip_extent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeoms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     out_image, out_transform = mask(\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mraster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_extent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_touched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_touched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    386\u001b[0m     \u001b[0mout_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/data_collection-Bxb0R3Gi/lib/python3.7/site-packages/rasterio/mask.py\u001b[0m in \u001b[0;36mmask\u001b[0;34m(dataset, shapes, all_touched, invert, nodata, filled, crop, pad, pad_width, indexes)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     out_image = dataset.read(\n\u001b[0;32m--> 194\u001b[0;31m         window=window, out_shape=out_shape, masked=True, indexes=indexes)\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mout_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mshape_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mrasterio/_io.pyx\u001b[0m in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mrasterio/_io.pyx\u001b[0m in \u001b[0;36mrasterio._io.DatasetReaderBase._read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/logging/__init__.py\u001b[0m in \u001b[0;36mdebug\u001b[0;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m         \"\"\"\n\u001b[1;32m   1358\u001b[0m         \u001b[0mLog\u001b[0m \u001b[0;34m'msg % args'\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mseverity\u001b[0m \u001b[0;34m'DEBUG'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "master_raster_path = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/Master_Rasters/msk_geotiff_full.tif\"\n",
    "output_dir = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/Polygon_Crops_Test/\"\n",
    "\n",
    "export_aoi_polygon_rasters(gdf,master_raster_path,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tif_to_jpg(in_dir,out_dir):\n",
    "    \n",
    "    \n",
    "    \n",
    "    in_dir = add_trailing_slash(in_dir)\n",
    "    \n",
    "    out_dir = add_trailing_slash(out_dir)\n",
    "    \n",
    "    # If the output folder doesn't exist, create it\n",
    "    \n",
    "    create_dir(out_dir)\n",
    "    \n",
    "\n",
    "    # Export Polygons from TIF to  JPEG\n",
    "\n",
    "    tif_list = glob(in_dir + \"*.tif\",recursive=True)\n",
    "    \n",
    "    for tif_path in tif_list:\n",
    "        base_filename = tif_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        im = Image.open(tif_path)\n",
    "        im.thumbnail(im.size)\n",
    "        im.save(out_dir + base_filename + \".jpg\", \"JPEG\", quality=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/Polygon_Crops/MSK/Individual_Polygons/TIF/\"\n",
    "out_dir = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/Polygon_Crops/MSK/Individual_Polygons/JPG_2\"\n",
    "\n",
    "tif_to_jpg(in_dir,out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pc_staging_conda",
   "language": "python",
   "name": "pc_staging_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
