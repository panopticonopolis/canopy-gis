{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel Hub Config\n",
    "\n",
    "from env_vars import sentinel_hub_instance_id\n",
    "from sentinelhub import SHConfig\n",
    "\n",
    "# Import Area of Interest List\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from scripts.mgrs import encode,LLtoUTM\n",
    "\n",
    "\n",
    "# Sentinel Hub Tile Look Up / Download\n",
    "\n",
    "from sentinelhub import WebFeatureService, BBox, CRS, DataSource, AwsTileRequest\n",
    "\n",
    "\n",
    "# Cloud Masking\n",
    "\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import earthpy.mask as em\n",
    "\n",
    "# Generate Product Detail DataFrame\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "# Sort / Organize Tiles by Individual Folders\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "# Extract Polygon crops from products\n",
    "\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import earthpy.spatial as es\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shub_connect(sentinel_hub_instance_id):\n",
    "\n",
    "    INSTANCE_ID = sentinel_hub_instance_id  \n",
    "\n",
    "    if INSTANCE_ID:\n",
    "        config = SHConfig()\n",
    "        config.instance_id = INSTANCE_ID\n",
    "    else:\n",
    "        config = None\n",
    "        \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = shub_connect(sentinel_hub_instance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_aois(csv_loc):    \n",
    "\n",
    "    df_labels = pd.read_csv(csv_loc)\n",
    "    df_labels = df_labels[[\"center-lat\",\"center-long\",\"polygon\"]][0:33]\n",
    "\n",
    "    polygons = []\n",
    "    for polygon in df_labels[\"polygon\"]:\n",
    "        polygons.append(json.loads(polygon)[\"coordinates\"])\n",
    "\n",
    "    coordinates = []\n",
    "    for items in polygons:\n",
    "        for item in items:\n",
    "            for lon_lat in item:\n",
    "                coordinates.append(lon_lat)\n",
    "\n",
    "    #bounding box\n",
    "\n",
    "    min_lon = min([i[0] for i in coordinates])\n",
    "    min_lat = min([i[1] for i in coordinates])\n",
    "    max_lon = max([i[0] for i in coordinates])\n",
    "    max_lat = max([i[1] for i in coordinates])\n",
    "\n",
    "    bounding_box = min_lon,min_lat,max_lon,max_lat\n",
    "\n",
    "\n",
    "    tiles = []\n",
    "    for ll in coordinates:\n",
    "        tiles.append(encode(LLtoUTM(ll[1],ll[0]),1)[:-2])\n",
    "\n",
    "    tiles = list(set(tiles))\n",
    "    return bounding_box,tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box,tile_list = import_aois(\"./data/labelled/labels_Misha_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shub_tiles(output_dir,bands=\"R10m/TCI\",search_time_interval = ('2019-01-01T00:00:00', '2020-12-31T23:59:59'),\n",
    "                   product_type = \"SENTINEL2_L2A\",bounding_box = bounding_box,tile_list = tile_list):\n",
    "    #Additional Params\n",
    "    bands = ['R10m/TCI']\n",
    "\n",
    "    #Misha's Tiles of Interest\n",
    "    search_bbox = BBox(bbox=bounding_box, crs=CRS.WGS84)\n",
    "\n",
    "    search_time_interval = ('2019-01-01T00:00:00', '2020-12-31T23:59:59')\n",
    "    wfs_iterator = WebFeatureService(\n",
    "        search_bbox,\n",
    "        search_time_interval,\n",
    "        data_source=DataSource.SENTINEL2_L2A,\n",
    "        maxcc=.05,\n",
    "        config=config\n",
    "    )\n",
    "    results = wfs_iterator.get_tiles()\n",
    "    df = pd.DataFrame(results, columns=['Tilename','Date','AmazonID'])\n",
    "    df_tiles_of_interest = df[df[\"Tilename\"].isin(tile_list)]\n",
    "    df2 = df_tiles_of_interest.groupby('Tilename').head(10)\n",
    "    output2 = list(df2.itertuples(index=False,name=None))\n",
    "    for tile in output2:\n",
    "        tile_name, time, aws_index = tile\n",
    "\n",
    "        #Download SAFE Files\n",
    "        request = AwsTileRequest(\n",
    "            tile=tile_name,\n",
    "            time=time,\n",
    "            bands = bands, \n",
    "            aws_index=aws_index,\n",
    "            data_folder=output_dir,\n",
    "            data_source=DataSource.SENTINEL2_L2A,\n",
    "            safe_format = True\n",
    "        )\n",
    "\n",
    "        request.save_data(redownload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shub_tiles(\"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud_mask_tci(src_dir):\n",
    "    \n",
    "    msk_file_path = glob(src_dir + \"*/*/MSK_CLDPRB_20m.jp2\")[0]\n",
    "    tci_file_path = glob(src_dir + \"*/IMG_DATA/R10m/*.jp2\")[0]\n",
    "    tci_filename = tci_file_path.split(\"/\")[-1]\n",
    "    output_tci_file_path = src_dir + \"/IMG_DATA/R10m/\" + \"processed_\" + tci_filename \n",
    "\n",
    "    nodatavalue = int(0)\n",
    "\n",
    "    with rio.open(tci_file_path) as sen_TCI_src:\n",
    "        sen_TCI = sen_TCI_src.read(masked=True)\n",
    "        sen_TCI_meta = sen_TCI_src.meta\n",
    "\n",
    "    with rio.open(msk_file_path) as sen_mask_src:\n",
    "        sen_mask_pre = sen_mask_src.read(1)\n",
    "        sen_mask = np.repeat(np.repeat(sen_mask_pre,2,axis=0),2,axis=1)\n",
    "\n",
    "    # All pixels above 0 probability will be classified as True\n",
    "\n",
    "    sen_mask_qa = sen_mask > 0\n",
    "\n",
    "\n",
    "    # Apply mask to source TCI file\n",
    "    if np.count_nonzero(sen_mask_qa) > 0:\n",
    "        sen_TCI_cl_free_nan = em.mask_pixels(sen_TCI, sen_mask_qa)\n",
    "        sen_TCI_cl_free_processed = np.ma.filled(sen_TCI_cl_free_nan, fill_value=nodatavalue)\n",
    "    else:\n",
    "        sen_TCI_c1_free_processed = sen_mask_qa\n",
    "\n",
    "\n",
    "    # Export cloud-masked TCI file\n",
    "    with rio.open(output_tci_file_path, 'w', **sen_TCI_meta) as outf:\n",
    "        outf.write(sen_TCI_cl_free_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask_tci_safe_list(tci_folder_list):\n",
    "    dir_list = glob(tci_folder_list + \"/*\" )\n",
    "    \n",
    "    \n",
    "    for directory in dir_list:\n",
    "        cloud_mask_tci(directory)\n",
    "        \n",
    "    print(f\"Applied masks to {len(dir_list)} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied masks to 4 products\n"
     ]
    }
   ],
   "source": [
    "tci_folder_list = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_raw\"\n",
    "apply_mask_tci_safe_list(tci_folder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_product_detail_df(input_dir):\n",
    "    \n",
    "    dirs = os.listdir(input_dir)\n",
    "\n",
    "    meta_data = []\n",
    "    for folder in dirs:\n",
    "        xml_loc = glob(input_dir + \"/\" + folder + \"/*.xml\")[0]\n",
    "        tree = ET.parse(xml_loc)\n",
    "        directory = [elem.text for elem in tree.iter() if \"MASK_FILENAME\" in elem.tag][0].split(\"/\")[1]\n",
    "        tile_id = directory.split(\"_\")[1]\n",
    "        filepath_partial = input_dir + \"/\" + directory + \"/IMG_DATA\" + \"/R10m\"\n",
    "        filepath = glob(filepath_partial + \"/processed*.jp2\")[0]\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        cloud_cover,no_data,unclassified = [elem.text for elem in tree.iter() if \"CLOUDY_PIXEL_PERCENTAGE\" in elem.tag \n",
    "                 or \"NODATA_PIXEL_PERCENTAGE\" in elem.tag or \"UNCLASSIFIED_PERCENTAGE\" in elem.tag]\n",
    "        meta_data.append([directory,tile_id,cloud_cover,no_data,unclassified,filename,filepath])\n",
    "    df = pd.DataFrame(meta_data,columns=[\"Directory\",\"Tile_Id\",\"Cloud Cover\",\"No Data Percentage\",\"Unclassified Percentage\",\"Filename\",\"Filepath\"])\n",
    "    df2 = df.sort_values(by=[\"Tile_Id\",\"Cloud Cover\",\"Unclassified Percentage\"],ignore_index=True)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_raw\"\n",
    "\n",
    "df = generate_product_detail_df(input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_masked_tiles(df,output_dir):\n",
    "\n",
    "\n",
    "    layer = 1\n",
    "    for index,row in df.iterrows(): \n",
    "        destination_dir = output_dir + str(layer)\n",
    "        output_file = destination_dir + \"/\" + row[\"Filename\"]\n",
    "\n",
    "        # Check if directory exists\n",
    "        if not os.path.isdir(destination_dir):\n",
    "            os.mkdir(destination_dir)\n",
    "\n",
    "        # Copy file to existing or new directory\n",
    "        copyfile(row[\"Filepath\"],output_file)\n",
    "\n",
    "        # Check if Tile_Id already exists in the directory - only necessary up until the last tile\n",
    "        if len(df) > index + 1:\n",
    "            if df.loc[index,\"Tile_Id\"] == df.loc[index + 1,\"Tile_Id\"]:\n",
    "                layer += 1\n",
    "            else:\n",
    "                layer = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_masked_tiles(df,\"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/test_ordered/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop to Polygon per Tile\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/Users/purgatorid/Documents/GitHub/canopy-gis/data_collection/data/labelled/labels_Misha_v2.csv\")\n",
    "df_labels = df[[\"center-lat\",\"center-long\",\"polygon\"]][0:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = []\n",
    "for polygon in df_labels[\"polygon\"]:\n",
    "    polygon_temp = []\n",
    "    for coordinates in json.loads(polygon)[\"coordinates\"]:\n",
    "        for coordinate in coordinates:\n",
    "            polygon_temp.append(tuple(coordinate))\n",
    "        polygons.append(Polygon(polygon_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_series = gpd.GeoSeries(polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(gdf_series,geometry=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[\"geometry\"] = gdf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.drop(columns=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_raster_v2 = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/Master_Rasters/msk_geotiff_full.tif\"\n",
    "polygon_list = gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_raster_file = rio.open(master_raster_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/purgatorid/Documents/GitHub/canopy-gis/data_collection/data/labelled/labels_Misha_v2.csv\")\n",
    "df_labels = df[[\"center-lat\",\"center-long\",\"polygon\"]][0:33]\n",
    "\n",
    "\n",
    "polygons = []\n",
    "for polygon in df_labels[\"polygon\"]:\n",
    "    polygon_temp = []\n",
    "    for coordinates in json.loads(polygon)[\"coordinates\"]:\n",
    "        for coordinate in coordinates:\n",
    "            polygon_temp.append(tuple(coordinate))\n",
    "        polygons.append(Polygon(polygon_temp))\n",
    "\n",
    "gdf_series = gpd.GeoSeries(polygons)\n",
    "gdf = gpd.GeoDataFrame(gdf_series,geometry=0)\n",
    "gdf[\"geometry\"] = gdf[0]\n",
    "gdf = gdf.drop(columns=[0])\n",
    "\n",
    "\n",
    "master_raster_path = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/Master_Rasters/msk_geotiff_full.tif\"\n",
    "src_raster_file = rio.open(master_raster_path)\n",
    "\n",
    "out_base_path = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/Polygon_Crops_Test/\"\n",
    "for index in range(gdf.shape[0]):\n",
    "    crop_extent = gdf.loc[[index],\"geometry\"]\n",
    "    \n",
    "    \n",
    "    raster_crop, raster_meta = es.crop_image(src_raster_file, crop_extent)\n",
    "        \n",
    "    # Update the metadata to have the new shape (x and y and affine information)\n",
    "    raster_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": raster_crop.shape[1],\n",
    "                     \"width\": raster_crop.shape[2],\n",
    "                     \"transform\": raster_meta[\"transform\"]})\n",
    "\n",
    "    # generate an extent for the newly cropped object for plotting\n",
    "    cr_ext = rio.transform.array_bounds(raster_meta['height'], \n",
    "                                                raster_meta['width'], \n",
    "                                                raster_meta['transform'])\n",
    "\n",
    "    bound_order = [0,2,1,3]\n",
    "    cr_extent = [cr_ext[b] for b in bound_order]\n",
    "    \n",
    "    # mask the nodata\n",
    "    raster_crop_ma = np.ma.masked_equal(raster_crop, 0) \n",
    "\n",
    "    \n",
    "    # output_path\n",
    "    outpath = out_base_path + str(index+1) + '.tif'\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.isdir(out_base_path):\n",
    "        os.mkdir(out_base_path)\n",
    "    \n",
    "    \n",
    "    # Export cloud-masked TCI file\n",
    "    with rio.open(outpath, 'w', **raster_meta) as outf:\n",
    "            outf.write(raster_crop_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_gdf(csv_loc):\n",
    "    '''\n",
    "    import manually created areas of interest csv\n",
    "    \n",
    "    output is an in-memory geo dataframe with one polygon AOI per row to be utilized for cropping master raster\n",
    "    \n",
    "    '''\n",
    "    df = pd.read_csv(csv_loc)\n",
    "    df_labels = df[[\"center-lat\",\"center-long\",\"polygon\"]][0:33]\n",
    "\n",
    "\n",
    "    polygons = []\n",
    "    for polygon in df_labels[\"polygon\"]:\n",
    "        polygon_temp = []\n",
    "        for coordinates in json.loads(polygon)[\"coordinates\"]:\n",
    "            for coordinate in coordinates:\n",
    "                polygon_temp.append(tuple(coordinate))\n",
    "            polygons.append(Polygon(polygon_temp))\n",
    "\n",
    "    gdf_series = gpd.GeoSeries(polygons)\n",
    "    gdf = gpd.GeoDataFrame(gdf_series,geometry=0)\n",
    "    gdf[\"geometry\"] = gdf[0]\n",
    "    gdf = gdf.drop(columns=[0])\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_loc = \"/Users/purgatorid/Documents/GitHub/canopy-gis/data_collection/data/labelled/labels_Misha_v2.csv\"\n",
    "\n",
    "gdf = csv_to_gdf(csv_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_aoi_polygon_rasters(gdf,master_raster_path,output_dir):\n",
    "\n",
    "    src_raster_file = rio.open(master_raster_path)\n",
    "    \n",
    "    for index in range(gdf.shape[0]):\n",
    "        crop_extent = gdf.loc[[index],\"geometry\"]\n",
    "\n",
    "\n",
    "        raster_crop, raster_meta = es.crop_image(src_raster_file, crop_extent)\n",
    "\n",
    "        # Update the metadata to have the new shape (x and y and affine information)\n",
    "        raster_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": raster_crop.shape[1],\n",
    "                         \"width\": raster_crop.shape[2],\n",
    "                         \"transform\": raster_meta[\"transform\"]})\n",
    "\n",
    "        # generate an extent for the newly cropped object for plotting\n",
    "        cr_ext = rio.transform.array_bounds(raster_meta['height'], \n",
    "                                                    raster_meta['width'], \n",
    "                                                    raster_meta['transform'])\n",
    "\n",
    "        bound_order = [0,2,1,3]\n",
    "        cr_extent = [cr_ext[b] for b in bound_order]\n",
    "\n",
    "        # mask the nodata\n",
    "        raster_crop_ma = np.ma.masked_equal(raster_crop, 0) \n",
    "\n",
    "\n",
    "        # output_path\n",
    "        outpath = out_base_path + str(index+1) + '.tif'\n",
    "\n",
    "        # Check if directory exists\n",
    "        if not os.path.isdir(out_base_path):\n",
    "            os.mkdir(out_base_path)\n",
    "\n",
    "\n",
    "        # Export cloud-masked TCI file\n",
    "        with rio.open(outpath, 'w', **raster_meta) as outf:\n",
    "                outf.write(raster_crop_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b712d91838e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/Polygon_Crops_Test/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexport_aoi_polygon_rasters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaster_raster_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-43e5cf1db1df>\u001b[0m in \u001b[0;36mexport_aoi_polygon_rasters\u001b[0;34m(gdf, master_raster_path, output_dir)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mraster_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraster_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_raster_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_extent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Update the metadata to have the new shape (x and y and affine information)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/data_collection-Bxb0R3Gi/lib/python3.7/site-packages/earthpy/spatial.py\u001b[0m in \u001b[0;36mcrop_image\u001b[0;34m(raster, geoms, all_touched)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mclip_extent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeoms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     out_image, out_transform = mask(\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mraster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_extent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_touched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_touched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    386\u001b[0m     \u001b[0mout_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/data_collection-Bxb0R3Gi/lib/python3.7/site-packages/rasterio/mask.py\u001b[0m in \u001b[0;36mmask\u001b[0;34m(dataset, shapes, all_touched, invert, nodata, filled, crop, pad, pad_width, indexes)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     out_image = dataset.read(\n\u001b[0;32m--> 194\u001b[0;31m         window=window, out_shape=out_shape, masked=True, indexes=indexes)\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mout_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mshape_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mrasterio/_io.pyx\u001b[0m in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mrasterio/_io.pyx\u001b[0m in \u001b[0;36mrasterio._io.DatasetReaderBase._read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/logging/__init__.py\u001b[0m in \u001b[0;36mdebug\u001b[0;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m         \"\"\"\n\u001b[1;32m   1358\u001b[0m         \u001b[0mLog\u001b[0m \u001b[0;34m'msg % args'\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mseverity\u001b[0m \u001b[0;34m'DEBUG'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "master_raster_path = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/Master_Rasters/msk_geotiff_full.tif\"\n",
    "output_dir = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Labelled/Tiles_v2_Misha/Polygon_Crops_Test/\"\n",
    "\n",
    "export_aoi_polygon_rasters(gdf,master_raster_path,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canopy-conda",
   "language": "python",
   "name": "canopy_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
